{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from data_gen import *\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "enc_units = 265\n",
    "feat_unit = 15\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, enc_units, feat_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(feat_units, activation=\"relu\", name=\"feature_output\")\n",
    "        self.fc2 = tf.keras.layers.Dense(enc_units, activation=\"relu\", name=\"state_out\")\n",
    "        \n",
    "    def call(self, w, f, hidden):\n",
    "        output, state = self.gru(w, initial_state=hidden)\n",
    "        feat = self.fc1(f)\n",
    "        state = tf.concat([state, feat], axis=1)\n",
    "        state = self.fc2(state)\n",
    "        return output, state, feat\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder(enc_units, feat_unit, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_x = [np.random.rand(batch_size, 15, 20).astype(np.float64), np.random.rand(batch_size, 32).astype(np.float64)]\n",
    "# sample_hidden = encoder.initialize_hidden_state()\n",
    "# s = tf.cast(sample_x[0], tf.float32)\n",
    "# k = tf.cast(sample_x[1], tf.float32)\n",
    "# sample_output, sample_hidden, sample_feat = encoder(s, k, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_layer = BahdanauAttention(10)\n",
    "# attention_result, attention_weights = attention_layer(sample_hidden, sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(28, activation=\"softmax\")\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output, feat):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = tf.concat([context_vector, x, feat], axis=-1)\n",
    "        x = tf.expand_dims(x, 1)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        x = self.fc(output)\n",
    "        return x, state#, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(enc_units, batch_size)\n",
    "encoder = Encoder(enc_units, feat_unit, batch_size)\n",
    "# sample_decoder_output, _, _ = decoder(tf.random.uniform((batch_size,  29)), sample_hidden, sample_output, sample_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(root, dec_input, feature, target, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden, feat = encoder(root, feature, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        for t in range(target.shape[1]):\n",
    "            predictions, dec_hidden = decoder(dec_input[:, t], dec_hidden, enc_output, feat)\n",
    "            loss += loss_function(target[:, t], predictions)\n",
    "\n",
    "        batch_loss = (loss / int(target.shape[1]))\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# data generator\n",
    "dg = DataGen(data=\"data/wol-aligned.txt\")\n",
    "\n",
    "# length of a word\n",
    "n_input_length = len(char2int)\n",
    "n_steps_in = dg.max_root_len\n",
    "n_steps_out = dg.max_output_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train data:  12898\n",
      "Steps: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Total train data: \", int(len(dg.words) * .016))\n",
    "batch_size = 128\n",
    "# number of batches to train\n",
    "n_batches = int(len(dg.words) * .016 / batch_size) \n",
    "print(\"Steps: {0}\".format(n_batches))\n",
    "# python generator to generate training data at each request\n",
    "# E.x word_matrix, feature = next(gen)\n",
    "gen = dg.rnn_gen_data(batch_size=batch_size, n_batches=n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.3343\n",
      "Epoch 1 Batch 10 Loss 1.8204\n",
      "Epoch 1 Batch 20 Loss 1.5072\n",
      "Epoch 1 Batch 30 Loss 1.4130\n",
      "Epoch 1 Batch 40 Loss 1.3196\n",
      "Epoch 1 Batch 50 Loss 1.2262\n",
      "Epoch 1 Batch 60 Loss 1.2502\n",
      "Epoch 1 Batch 70 Loss 1.1243\n",
      "Epoch 1 Batch 80 Loss 1.1454\n",
      "Epoch 1 Batch 90 Loss 1.0952\n",
      "Epoch 1 Loss 1.3838\n",
      "Time taken for 1 epoch 199.19089651107788 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.0044\n",
      "Epoch 2 Batch 10 Loss 1.0440\n",
      "Epoch 2 Batch 20 Loss 0.9705\n",
      "Epoch 2 Batch 30 Loss 0.9617\n",
      "Epoch 2 Batch 40 Loss 0.8985\n",
      "Epoch 2 Batch 50 Loss 0.8359\n",
      "Epoch 2 Batch 60 Loss 0.8424\n",
      "Epoch 2 Batch 70 Loss 0.7341\n",
      "Epoch 2 Batch 80 Loss 0.7336\n",
      "Epoch 2 Batch 90 Loss 0.6922\n",
      "Epoch 2 Loss 0.8523\n",
      "Time taken for 1 epoch 47.17649483680725 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.6097\n",
      "Epoch 3 Batch 10 Loss 0.6334\n",
      "Epoch 3 Batch 20 Loss 0.5864\n",
      "Epoch 3 Batch 30 Loss 0.5638\n",
      "Epoch 3 Batch 40 Loss 0.5104\n",
      "Epoch 3 Batch 50 Loss 0.4964\n",
      "Epoch 3 Batch 60 Loss 0.4938\n",
      "Epoch 3 Batch 70 Loss 0.3998\n",
      "Epoch 3 Batch 80 Loss 0.4314\n",
      "Epoch 3 Batch 90 Loss 0.4289\n",
      "Epoch 3 Loss 0.5053\n",
      "Time taken for 1 epoch 47.4312858581543 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.3430\n",
      "Epoch 4 Batch 10 Loss 0.3564\n",
      "Epoch 4 Batch 20 Loss 0.3454\n",
      "Epoch 4 Batch 30 Loss 0.3203\n",
      "Epoch 4 Batch 40 Loss 0.2579\n",
      "Epoch 4 Batch 50 Loss 0.2858\n",
      "Epoch 4 Batch 60 Loss 0.3482\n",
      "Epoch 4 Batch 70 Loss 0.2402\n",
      "Epoch 4 Batch 80 Loss 0.2252\n",
      "Epoch 4 Batch 90 Loss 0.2180\n",
      "Epoch 4 Loss 0.2876\n",
      "Time taken for 1 epoch 48.669498443603516 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1884\n",
      "Epoch 5 Batch 10 Loss 0.1929\n",
      "Epoch 5 Batch 20 Loss 0.2214\n",
      "Epoch 5 Batch 30 Loss 0.1881\n",
      "Epoch 5 Batch 40 Loss 0.1563\n",
      "Epoch 5 Batch 50 Loss 0.1523\n",
      "Epoch 5 Batch 60 Loss 0.1668\n",
      "Epoch 5 Batch 70 Loss 0.1273\n",
      "Epoch 5 Batch 80 Loss 0.1341\n",
      "Epoch 5 Batch 90 Loss 0.1278\n",
      "Epoch 5 Loss 0.1642\n",
      "Time taken for 1 epoch 48.60524916648865 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1194\n",
      "Epoch 6 Batch 10 Loss 0.1258\n",
      "Epoch 6 Batch 20 Loss 0.1299\n",
      "Epoch 6 Batch 30 Loss 0.1234\n",
      "Epoch 6 Batch 40 Loss 0.1000\n",
      "Epoch 6 Batch 50 Loss 0.1050\n",
      "Epoch 6 Batch 60 Loss 0.1263\n",
      "Epoch 6 Batch 70 Loss 0.1547\n",
      "Epoch 6 Batch 80 Loss 0.1093\n",
      "Epoch 6 Batch 90 Loss 0.0971\n",
      "Epoch 6 Loss 0.1171\n",
      "Time taken for 1 epoch 46.8713595867157 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0828\n",
      "Epoch 7 Batch 10 Loss 0.0815\n",
      "Epoch 7 Batch 20 Loss 0.0867\n",
      "Epoch 7 Batch 30 Loss 0.0788\n",
      "Epoch 7 Batch 40 Loss 0.0635\n",
      "Epoch 7 Batch 50 Loss 0.0655\n",
      "Epoch 7 Batch 60 Loss 0.0754\n",
      "Epoch 7 Batch 70 Loss 0.0636\n",
      "Epoch 7 Batch 80 Loss 0.0594\n",
      "Epoch 7 Batch 90 Loss 0.0701\n",
      "Epoch 7 Loss 0.0722\n",
      "Time taken for 1 epoch 46.561362981796265 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0612\n",
      "Epoch 8 Batch 10 Loss 0.0554\n",
      "Epoch 8 Batch 20 Loss 0.0640\n",
      "Epoch 8 Batch 30 Loss 0.0585\n",
      "Epoch 8 Batch 40 Loss 0.0446\n",
      "Epoch 8 Batch 50 Loss 0.0410\n",
      "Epoch 8 Batch 60 Loss 0.0470\n",
      "Epoch 8 Batch 70 Loss 0.0373\n",
      "Epoch 8 Batch 80 Loss 0.0454\n",
      "Epoch 8 Batch 90 Loss 0.0417\n",
      "Epoch 8 Loss 0.0508\n",
      "Time taken for 1 epoch 46.47800159454346 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0339\n",
      "Epoch 9 Batch 10 Loss 0.0376\n",
      "Epoch 9 Batch 20 Loss 0.0406\n",
      "Epoch 9 Batch 30 Loss 0.0389\n",
      "Epoch 9 Batch 40 Loss 0.0298\n",
      "Epoch 9 Batch 50 Loss 0.0294\n",
      "Epoch 9 Batch 60 Loss 0.0318\n",
      "Epoch 9 Batch 70 Loss 0.0263\n",
      "Epoch 9 Batch 80 Loss 0.0313\n",
      "Epoch 9 Batch 90 Loss 0.0347\n",
      "Epoch 9 Loss 0.0452\n",
      "Time taken for 1 epoch 46.373504400253296 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.1503\n",
      "Epoch 10 Batch 10 Loss 0.0732\n",
      "Epoch 10 Batch 20 Loss 0.0588\n",
      "Epoch 10 Batch 30 Loss 0.0451\n",
      "Epoch 10 Batch 40 Loss 0.0308\n",
      "Epoch 10 Batch 50 Loss 0.0265\n",
      "Epoch 10 Batch 60 Loss 0.0310\n",
      "Epoch 10 Batch 70 Loss 0.0221\n",
      "Epoch 10 Batch 80 Loss 0.0221\n",
      "Epoch 10 Batch 90 Loss 0.0263\n",
      "Epoch 10 Loss 0.0431\n",
      "Time taken for 1 epoch 46.451510190963745 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0211\n",
      "Epoch 11 Batch 10 Loss 0.0213\n",
      "Epoch 11 Batch 20 Loss 0.0245\n",
      "Epoch 11 Batch 30 Loss 0.0231\n",
      "Epoch 11 Batch 40 Loss 0.0154\n",
      "Epoch 11 Batch 50 Loss 0.0159\n",
      "Epoch 11 Batch 60 Loss 0.0200\n",
      "Epoch 11 Batch 70 Loss 0.0163\n",
      "Epoch 11 Batch 80 Loss 0.0143\n",
      "Epoch 11 Batch 90 Loss 0.0197\n",
      "Epoch 11 Loss 0.0202\n",
      "Time taken for 1 epoch 46.36736464500427 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0161\n",
      "Epoch 12 Batch 10 Loss 0.0171\n",
      "Epoch 12 Batch 20 Loss 0.0191\n",
      "Epoch 12 Batch 30 Loss 0.0172\n",
      "Epoch 12 Batch 40 Loss 0.0123\n",
      "Epoch 12 Batch 50 Loss 0.0113\n",
      "Epoch 12 Batch 60 Loss 0.0154\n",
      "Epoch 12 Batch 70 Loss 0.0122\n",
      "Epoch 12 Batch 80 Loss 0.0106\n",
      "Epoch 12 Batch 90 Loss 0.0170\n",
      "Epoch 12 Loss 0.0155\n",
      "Time taken for 1 epoch 46.435311794281006 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0123\n",
      "Epoch 13 Batch 10 Loss 0.0139\n",
      "Epoch 13 Batch 20 Loss 0.0153\n",
      "Epoch 13 Batch 30 Loss 0.0130\n",
      "Epoch 13 Batch 40 Loss 0.0097\n",
      "Epoch 13 Batch 50 Loss 0.0088\n",
      "Epoch 13 Batch 60 Loss 0.0122\n",
      "Epoch 13 Batch 70 Loss 0.0096\n",
      "Epoch 13 Batch 80 Loss 0.0084\n",
      "Epoch 13 Batch 90 Loss 0.0139\n",
      "Epoch 13 Loss 0.0124\n",
      "Time taken for 1 epoch 46.40674901008606 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0099\n",
      "Epoch 14 Batch 10 Loss 0.0114\n",
      "Epoch 14 Batch 20 Loss 0.0132\n",
      "Epoch 14 Batch 30 Loss 0.0101\n",
      "Epoch 14 Batch 40 Loss 0.0084\n",
      "Epoch 14 Batch 50 Loss 0.0069\n",
      "Epoch 14 Batch 60 Loss 0.0095\n",
      "Epoch 14 Batch 70 Loss 0.0087\n",
      "Epoch 14 Batch 80 Loss 0.0064\n",
      "Epoch 14 Batch 90 Loss 0.0099\n",
      "Epoch 14 Loss 0.0104\n",
      "Time taken for 1 epoch 46.44590663909912 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0090\n",
      "Epoch 15 Batch 10 Loss 0.0087\n",
      "Epoch 15 Batch 20 Loss 0.0100\n",
      "Epoch 15 Batch 30 Loss 0.0087\n",
      "Epoch 15 Batch 40 Loss 0.0080\n",
      "Epoch 15 Batch 50 Loss 0.0069\n",
      "Epoch 15 Batch 60 Loss 0.0093\n",
      "Epoch 15 Batch 70 Loss 0.0083\n",
      "Epoch 15 Batch 80 Loss 0.0055\n",
      "Epoch 15 Batch 90 Loss 0.0080\n",
      "Epoch 15 Loss 0.0091\n",
      "Time taken for 1 epoch 46.438257694244385 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0089\n",
      "Epoch 16 Batch 10 Loss 1.6912\n",
      "Epoch 16 Batch 20 Loss 0.3551\n",
      "Epoch 16 Batch 30 Loss 0.2011\n",
      "Epoch 16 Batch 40 Loss 0.1133\n",
      "Epoch 16 Batch 50 Loss 0.0878\n",
      "Epoch 16 Batch 60 Loss 0.0861\n",
      "Epoch 16 Batch 70 Loss 0.0626\n",
      "Epoch 16 Batch 80 Loss 0.0548\n",
      "Epoch 16 Batch 90 Loss 0.0481\n",
      "Epoch 16 Loss 0.1769\n",
      "Time taken for 1 epoch 46.44495701789856 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0417\n",
      "Epoch 17 Batch 10 Loss 0.0421\n",
      "Epoch 17 Batch 20 Loss 0.0461\n",
      "Epoch 17 Batch 30 Loss 0.0367\n",
      "Epoch 17 Batch 40 Loss 0.0294\n",
      "Epoch 17 Batch 50 Loss 0.0296\n",
      "Epoch 17 Batch 60 Loss 0.0301\n",
      "Epoch 17 Batch 70 Loss 0.0272\n",
      "Epoch 17 Batch 80 Loss 0.0216\n",
      "Epoch 17 Batch 90 Loss 0.0239\n",
      "Epoch 17 Loss 0.0333\n",
      "Time taken for 1 epoch 46.41437578201294 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0204\n",
      "Epoch 18 Batch 10 Loss 0.0225\n",
      "Epoch 18 Batch 20 Loss 0.0266\n",
      "Epoch 18 Batch 30 Loss 0.0213\n",
      "Epoch 18 Batch 40 Loss 0.0192\n",
      "Epoch 18 Batch 50 Loss 0.0179\n",
      "Epoch 18 Batch 60 Loss 0.0191\n",
      "Epoch 18 Batch 70 Loss 0.0191\n",
      "Epoch 18 Batch 80 Loss 0.0132\n",
      "Epoch 18 Batch 90 Loss 0.0152\n",
      "Epoch 18 Loss 0.0202\n",
      "Time taken for 1 epoch 46.469459533691406 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0143\n",
      "Epoch 19 Batch 10 Loss 0.0147\n",
      "Epoch 19 Batch 20 Loss 0.0195\n",
      "Epoch 19 Batch 30 Loss 0.0153\n",
      "Epoch 19 Batch 40 Loss 0.0150\n",
      "Epoch 19 Batch 50 Loss 0.0124\n",
      "Epoch 19 Batch 60 Loss 0.0125\n",
      "Epoch 19 Batch 70 Loss 0.0131\n",
      "Epoch 19 Batch 80 Loss 0.0095\n",
      "Epoch 19 Batch 90 Loss 0.0111\n",
      "Epoch 19 Loss 0.0146\n",
      "Time taken for 1 epoch 46.41551971435547 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0112\n",
      "Epoch 20 Batch 10 Loss 0.0105\n",
      "Epoch 20 Batch 20 Loss 0.0151\n",
      "Epoch 20 Batch 30 Loss 0.0116\n",
      "Epoch 20 Batch 40 Loss 0.0116\n",
      "Epoch 20 Batch 50 Loss 0.0097\n",
      "Epoch 20 Batch 60 Loss 0.0094\n",
      "Epoch 20 Batch 70 Loss 0.0097\n",
      "Epoch 20 Batch 80 Loss 0.0072\n",
      "Epoch 20 Batch 90 Loss 0.0096\n",
      "Epoch 20 Loss 0.0117\n",
      "Time taken for 1 epoch 46.481043577194214 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0088\n",
      "Epoch 21 Batch 10 Loss 0.0082\n",
      "Epoch 21 Batch 20 Loss 0.0136\n",
      "Epoch 21 Batch 30 Loss 0.0097\n",
      "Epoch 21 Batch 40 Loss 0.0086\n",
      "Epoch 21 Batch 50 Loss 0.0082\n",
      "Epoch 21 Batch 60 Loss 0.0077\n",
      "Epoch 21 Batch 70 Loss 0.0081\n",
      "Epoch 21 Batch 80 Loss 0.0060\n",
      "Epoch 21 Batch 90 Loss 0.0085\n",
      "Epoch 21 Loss 0.0096\n",
      "Time taken for 1 epoch 46.966182231903076 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0079\n",
      "Epoch 22 Batch 10 Loss 0.0069\n",
      "Epoch 22 Batch 20 Loss 0.0108\n",
      "Epoch 22 Batch 30 Loss 0.0083\n",
      "Epoch 22 Batch 40 Loss 0.0071\n",
      "Epoch 22 Batch 50 Loss 0.0074\n",
      "Epoch 22 Batch 60 Loss 0.0088\n",
      "Epoch 22 Batch 70 Loss 0.0068\n",
      "Epoch 22 Batch 80 Loss 0.0051\n",
      "Epoch 22 Batch 90 Loss 0.0066\n",
      "Epoch 22 Loss 0.0080\n",
      "Time taken for 1 epoch 46.47945213317871 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0081\n",
      "Epoch 23 Batch 10 Loss 0.0060\n",
      "Epoch 23 Batch 20 Loss 0.0086\n",
      "Epoch 23 Batch 30 Loss 0.0073\n",
      "Epoch 23 Batch 40 Loss 0.0064\n",
      "Epoch 23 Batch 50 Loss 0.0057\n",
      "Epoch 23 Batch 60 Loss 0.0134\n",
      "Epoch 23 Batch 70 Loss 0.0067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Batch 80 Loss 0.0053\n",
      "Epoch 23 Batch 90 Loss 0.0056\n",
      "Epoch 23 Loss 0.0078\n",
      "Time taken for 1 epoch 46.43367004394531 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0057\n",
      "Epoch 24 Batch 10 Loss 0.0057\n",
      "Epoch 24 Batch 20 Loss 0.0089\n",
      "Epoch 24 Batch 30 Loss 0.0081\n",
      "Epoch 24 Batch 40 Loss 0.0061\n",
      "Epoch 24 Batch 50 Loss 0.0044\n",
      "Epoch 24 Batch 60 Loss 0.0049\n",
      "Epoch 24 Batch 70 Loss 0.0055\n",
      "Epoch 24 Batch 80 Loss 0.0036\n",
      "Epoch 24 Batch 90 Loss 0.0054\n",
      "Epoch 24 Loss 0.0064\n",
      "Time taken for 1 epoch 46.47045922279358 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0054\n",
      "Epoch 25 Batch 10 Loss 0.0053\n",
      "Epoch 25 Batch 20 Loss 0.0065\n",
      "Epoch 25 Batch 30 Loss 0.0067\n",
      "Epoch 25 Batch 40 Loss 0.0061\n",
      "Epoch 25 Batch 50 Loss 0.0057\n",
      "Epoch 25 Batch 60 Loss 0.0037\n",
      "Epoch 25 Batch 70 Loss 0.0055\n",
      "Epoch 25 Batch 80 Loss 0.0036\n",
      "Epoch 25 Batch 90 Loss 0.0045\n",
      "Epoch 25 Loss 0.0056\n",
      "Time taken for 1 epoch 46.39604473114014 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step in range(n_batches):\n",
    "        [root, dec_in, feat], y = next(gen)\n",
    "#         root = root.astype(np.float32)# tf.cast(root, tf.float32)\n",
    "#         dec_in = tf.cast(dec_in, tf.float32)\n",
    "#         feat = tf.cast(feat, tf.float32)\n",
    "#         y = tf.cast(y, tf.float32)\n",
    "        batch_loss = train_step(root, dec_in, feat, y, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if step % (n_batches // 10) == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     step,\n",
    "                                                     batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "#     if (epoch + 1) % 2 == 0:\n",
    "#         checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / n_batches))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664\n"
     ]
    }
   ],
   "source": [
    "test_n_batches, test_batch_size =  int(len(dg.words) * .00208 / batch_size), batch_size  \n",
    "print(test_n_batches * test_batch_size)\n",
    "# test_n_batches, test_batch_size = 30, 10\n",
    "\n",
    "# data generator for test data\n",
    "test_gen = dg.rnn_gen_data(batch_size=test_batch_size, n_batches=test_n_batches, trainset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(infenc, infdec, inputs, n_steps, cardinality):\n",
    "    # encode\n",
    "    root = tf.cast(inputs[0], tf.float32)\n",
    "#     dec_in = tf.cast(inputs[1], tf.float32)\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    hidden = [tf.zeros((1, enc_units), dtype=tf.float32)]\n",
    "    feat = tf.cast(inputs[1], tf.float32)\n",
    "#     print(root.shape, feat.shape)\n",
    "    outputs, state, feat = encoder(root, feat, None)\n",
    "    \n",
    "    # start of sequence input\n",
    "    start = [0.0 for _ in range(cardinality)]\n",
    "#     start[0] = 1\n",
    "    target_seq = np.array(start).reshape(1, cardinality)\n",
    "    # collect predictions\n",
    "    output = list()\n",
    "#     state = tf.expand_dims(state, 1)\n",
    "    for t in range(n_steps):\n",
    "        # predict next char\n",
    "        \n",
    "        target_seq = tf.cast(target_seq, tf.float32)\n",
    "#         print(target_seq.shape, state.shape, outputs.shape, feat.shape)\n",
    "        yhat, h = decoder(target_seq, state, outputs, feat)\n",
    "        # store prediction\n",
    "#         print(yhat.shape)\n",
    "        output.append(np.array(yhat))\n",
    "        # update state\n",
    "        state = h\n",
    "        # update target sequence\n",
    "        target_seq = yhat\n",
    "    return np.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Accuracy: 89.42%\n"
     ]
    }
   ],
   "source": [
    "# shows sample examples and calculates accuracy\n",
    "\n",
    "total, correct = 0, 0\n",
    "in_word = 0\n",
    "sims = []\n",
    "for b in range(test_n_batches):\n",
    "    # get data from test data generator\n",
    "    [X1, X2, X3], y = next(test_gen)\n",
    "    for j in range(test_batch_size):\n",
    "        word_features = X3[j].reshape((1, X3.shape[1])) \n",
    "        root_word_matrix = X1[j].reshape((1, X1.shape[1], X1.shape[2]))\n",
    "#         word_index = X4[j].reshape((1, X4.shape[1]))\n",
    "        # predicts the target word given root word and features\n",
    "        \n",
    "        target = predict(encoder, decoder, [root_word_matrix, word_features], n_steps_out, n_input_length)\n",
    "        root = ''.join(dg.one_hot_decode(X1[j]))#.replace('&', ' ')\n",
    "        word = ''.join(dg.one_hot_decode(y[j]))#.replace('&', ' ')\n",
    "        targetS = ''.join(dg.one_hot_decode(target))#.replace('&', ' ')\n",
    "#         sims.append(dg.word_sim(word, targetS))\n",
    "        \n",
    "        # checks if the predicted and the real words are equal\n",
    "        if dg.one_hot_decode(y[j]) == dg.one_hot_decode(target):\n",
    "            correct += 1\n",
    "#         else:\n",
    "#             print(root, word.split('&')[0], '\\t\\t', targetS.split('&')[0])\n",
    "#         if root.strip() in targetS.strip():\n",
    "#             in_word += 1\n",
    "#     print(b, root, word, targetS)\n",
    "    total += test_batch_size\n",
    "    \n",
    "\n",
    "print('Exact Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12898 1664 16  89.66% 82.69% 93.63%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12898 1664 25 93.54 89.84% 89.42%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
