{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from data_gen import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "enc_units = 100\n",
    "feat_unit = 15\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, enc_units, feat_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "#         self.cat = tf.keras.layers.Concatenate(axis=1)\n",
    "        self.fc1 = tf.keras.layers.Dense(feat_units, activation=\"relu\", name=\"feature_output\")\n",
    "#         self.fc2 = tf.keras.layers.Dense(embedding_dim, activation=\"relu\", name=\"feature_output\")\n",
    "\n",
    "    def call(self, w, f, hidden):\n",
    "#         w, f = x\n",
    "#         print(self.gru)\n",
    "        output, state = self.gru(w, initial_state=hidden)\n",
    "        feat = self.fc1(f)\n",
    "#         x = tf.concat([state, feat], axis=1)\n",
    "#         state = self.fc2(x)\n",
    "        return output, state, feat\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size, enc_units, feat_unit, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_x = [np.random.rand(batch_size, 15, 20).astype(np.float64), np.random.rand(batch_size, 32).astype(np.float64)]\n",
    "# sample_hidden = encoder.initialize_hidden_state()\n",
    "# s = tf.cast(sample_x[0], tf.float32)\n",
    "# k = tf.cast(sample_x[1], tf.float32)\n",
    "# sample_output, sample_hidden, sample_feat = encoder(s, k, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_layer = BahdanauAttention(10)\n",
    "# attention_result, attention_weights = attention_layer(sample_hidden, sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(28)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output, feat):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "#         x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "#         print(context_vector.shape, x.shape, feat.shape)\n",
    "        \n",
    "        x = tf.concat([context_vector, x, feat], axis=-1)\n",
    "        x = tf.expand_dims(x, 1)\n",
    "#         x = tf.reshape(x, (-1, 1, x.shape[-1]))\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "#         print(output.shape)\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "#         print(x.shape)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(embed_size, enc_units, batch_size)\n",
    "\n",
    "# sample_decoder_output, _, _ = decoder(tf.random.uniform((batch_size,  29)), sample_hidden, sample_output, sample_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "#     mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "#     mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#     loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(root, dec_input, feature, target, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden, feat = encoder(root, feature, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "#         dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(target.shape[1]):\n",
    "          # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input[:, t], dec_hidden, enc_output, feat)\n",
    "    #         print(predictions.shape, target[:, t].shape)\n",
    "            loss += loss_function(target[:, t], predictions)\n",
    "    #         print(loss.shape)\n",
    "          # using teacher forcing\n",
    "    #         dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "        batch_loss = (loss / int(target.shape[1]))\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "#     print(batch_loss)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# data generator\n",
    "dg = DataGen(data=\"data/wol-aligned.txt\")\n",
    "\n",
    "# length of a word\n",
    "n_input_length = len(char2int)\n",
    "n_steps_in = dg.max_root_len\n",
    "n_steps_out = dg.max_output_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train data:  564292.3999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Total train data: \", len(dg.words) * .7)\n",
    "batch_size = 128\n",
    "# number of batches to train\n",
    "n_batches = int(len(dg.words) * .7 / batch_size) \n",
    "\n",
    "# python generator to generate training data at each request\n",
    "# E.x word_matrix, feature = next(gen)\n",
    "gen = dg.rnn_gen_data(batch_size=batch_size, n_batches=n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.3411\n",
      "Epoch 1 Batch 1000 Loss 0.2081\n",
      "Epoch 1 Batch 2000 Loss 0.0659\n",
      "Epoch 1 Batch 3000 Loss 0.0255\n",
      "Epoch 1 Batch 4000 Loss 0.0283\n",
      "Epoch 1 Loss 0.2154\n",
      "Time taken for 1 epoch 685.1494975090027 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.0126\n",
      "Epoch 2 Batch 1000 Loss 0.0073\n",
      "Epoch 2 Batch 2000 Loss 0.0098\n",
      "Epoch 2 Batch 3000 Loss 0.0048\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step in range(n_batches):\n",
    "        [root, dec_in, feat], y = next(gen)\n",
    "        root = tf.cast(root, tf.float32)\n",
    "        dec_in = tf.cast(dec_in, tf.float32)\n",
    "        feat = tf.cast(feat, tf.float32)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "        batch_loss = train_step(root, dec_in, feat, y, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     step,\n",
    "                                                     batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / n_batches))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_n_batches, test_batch_size =  int(len(dg.words) * .7 / batch_size), batch_size  \n",
    "test_n_batches, test_batch_size = 30, 100 \n",
    "\n",
    "# data generator for test data\n",
    "test_gen = dg.rnn_gen_data(batch_size=test_batch_size, n_batches=test_n_batches, trainset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(infenc, infdec, inputs, n_steps, cardinality):\n",
    "    # encode\n",
    "    root = tf.cast(inputs[0], tf.float32)\n",
    "#     dec_in = tf.cast(inputs[1], tf.float32)\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    hidden = [tf.zeros((1, 100), dtype=tf.float32)]\n",
    "    feat = tf.cast(inputs[1], tf.float32)\n",
    "#     print(root.shape, feat.shape)\n",
    "    outputs, state, feat = encoder(root, feat, hidden)\n",
    "    \n",
    "    # start of sequence input\n",
    "    start = [0.0 for _ in range(cardinality)]\n",
    "#     start[0] = 1\n",
    "    target_seq = np.array(start).reshape(1, cardinality)\n",
    "    # collect predictions\n",
    "    output = list()\n",
    "#     state = tf.expand_dims(state, 1)\n",
    "    for t in range(n_steps):\n",
    "        # predict next char\n",
    "        \n",
    "        target_seq = tf.cast(target_seq, tf.float32)\n",
    "#         print(target_seq.shape, state.shape, outputs.shape, feat.shape)\n",
    "        yhat, h, att_w = infdec(target_seq, state, outputs, feat)\n",
    "        # store prediction\n",
    "#         print(yhat.shape)\n",
    "        output.append(np.array(yhat))\n",
    "        # update state\n",
    "        state = h\n",
    "        # update target sequence\n",
    "        target_seq = yhat\n",
    "    return np.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows sample examples and calculates accuracy\n",
    "\n",
    "total, correct = 0, 0\n",
    "in_word = 0\n",
    "sims = []\n",
    "for b in range(test_n_batches):\n",
    "    # get data from test data generator\n",
    "    [X1, X2, X3], y = next(test_gen)\n",
    "    for j in range(test_batch_size):\n",
    "        word_features = X3[j].reshape((1, X3.shape[1])) \n",
    "        root_word_matrix = X1[j].reshape((1, X1.shape[1], X1.shape[2]))\n",
    "#         word_index = X4[j].reshape((1, X4.shape[1]))\n",
    "        # predicts the target word given root word and features\n",
    "        \n",
    "        target = predict(encoder, decoder, [root_word_matrix, word_features], n_steps_out, n_input_length)\n",
    "        root = ''.join(dg.one_hot_decode(X1[j]))#.replace('&', ' ')\n",
    "        word = ''.join(dg.one_hot_decode(y[j]))#.replace('&', ' ')\n",
    "        targetS = ''.join(dg.one_hot_decode(target))#.replace('&', ' ')\n",
    "#         sims.append(dg.word_sim(word, targetS))\n",
    "        \n",
    "        # checks if the predicted and the real words are equal\n",
    "        if dg.one_hot_decode(y[j]) == dg.one_hot_decode(target):\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(root, word.split('&')[0], '\\t\\t', targetS.split('&')[0])\n",
    "#         if root.strip() in targetS.strip():\n",
    "#             in_word += 1\n",
    "#     print(b, root, word, targetS)\n",
    "    total += test_batch_size\n",
    "    \n",
    "\n",
    "print('Exact Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
