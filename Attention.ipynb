{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from data_gen import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "enc_units = 100\n",
    "feat_unit = 15\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, enc_units, feat_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "#         self.cat = tf.keras.layers.Concatenate(axis=1)\n",
    "        self.fc1 = tf.keras.layers.Dense(feat_units, activation=\"relu\", name=\"feature_output\")\n",
    "#         self.fc2 = tf.keras.layers.Dense(embedding_dim, activation=\"relu\", name=\"feature_output\")\n",
    "\n",
    "    def call(self, w, f, hidden):\n",
    "#         w, f = x\n",
    "#         print(self.gru)\n",
    "        output, state = self.gru(w, initial_state=hidden)\n",
    "        feat = self.fc1(f)\n",
    "#         x = tf.concat([state, feat], axis=1)\n",
    "#         state = self.fc2(x)\n",
    "        return output, state, feat\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size, enc_units, feat_unit, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_x = [np.random.rand(batch_size, 15, 20).astype(np.float64), np.random.rand(batch_size, 32).astype(np.float64)]\n",
    "# sample_hidden = encoder.initialize_hidden_state()\n",
    "# s = tf.cast(sample_x[0], tf.float32)\n",
    "# k = tf.cast(sample_x[1], tf.float32)\n",
    "# sample_output, sample_hidden, sample_feat = encoder(s, k, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_layer = BahdanauAttention(10)\n",
    "# attention_result, attention_weights = attention_layer(sample_hidden, sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(28)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output, feat):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "#         x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "#         print(context_vector.shape, x.shape, feat.shape)\n",
    "        \n",
    "        x = tf.concat([context_vector, x, feat], axis=-1)\n",
    "        x = tf.expand_dims(x, 1)\n",
    "#         x = tf.reshape(x, (-1, 1, x.shape[-1]))\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "#         print(output.shape)\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "#         print(x.shape)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(embed_size, enc_units, batch_size)\n",
    "\n",
    "# sample_decoder_output, _, _ = decoder(tf.random.uniform((batch_size,  29)), sample_hidden, sample_output, sample_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "#     mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "#     mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#     loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(root, dec_input, feature, target, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden, feat = encoder(root, feature, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "#         dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(target.shape[1]):\n",
    "          # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input[:, t], dec_hidden, enc_output, feat)\n",
    "    #         print(predictions.shape, target[:, t].shape)\n",
    "            loss += loss_function(target[:, t], predictions)\n",
    "    #         print(loss.shape)\n",
    "          # using teacher forcing\n",
    "    #         dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "        batch_loss = (loss / int(target.shape[1]))\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "#     print(batch_loss)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# data generator\n",
    "dg = DataGen(data=\"data/wol-aligned.txt\")\n",
    "\n",
    "# length of a word\n",
    "n_input_length = len(char2int)\n",
    "n_steps_in = dg.max_root_len\n",
    "n_steps_out = dg.max_output_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train data:  564292.3999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Total train data: \", len(dg.words) * .7)\n",
    "batch_size = 128\n",
    "# number of batches to train\n",
    "n_batches = int(len(dg.words) * .07 / batch_size) \n",
    "\n",
    "# python generator to generate training data at each request\n",
    "# E.x word_matrix, feature = next(gen)\n",
    "gen = dg.rnn_gen_data(batch_size=batch_size, n_batches=n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.3102\n",
      "Epoch 1 Loss 1.1777\n",
      "Time taken for 1 epoch 99.08928632736206 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.5769\n",
      "Epoch 2 Loss 0.3429\n",
      "Time taken for 1 epoch 64.46999144554138 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1870\n",
      "Epoch 3 Loss 0.1559\n",
      "Time taken for 1 epoch 71.61031794548035 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1147\n",
      "Epoch 4 Loss 0.0966\n",
      "Time taken for 1 epoch 64.03012585639954 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0782\n",
      "Epoch 5 Loss 0.0639\n",
      "Time taken for 1 epoch 63.63485813140869 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step in range(n_batches):\n",
    "        [root, dec_in, feat], y = next(gen)\n",
    "        root = tf.cast(root, tf.float32)\n",
    "        dec_in = tf.cast(dec_in, tf.float32)\n",
    "        feat = tf.cast(feat, tf.float32)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "        batch_loss = train_step(root, dec_in, feat, y, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     step,\n",
    "                                                     batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "#     if (epoch + 1) % 2 == 0:\n",
    "#         checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / n_batches))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_n_batches, test_batch_size =  int(len(dg.words) * .7 / batch_size), batch_size  \n",
    "test_n_batches, test_batch_size = 30, 100 \n",
    "\n",
    "# data generator for test data\n",
    "test_gen = dg.rnn_gen_data(batch_size=test_batch_size, n_batches=test_n_batches, trainset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(infenc, infdec, inputs, n_steps, cardinality):\n",
    "    # encode\n",
    "    root = tf.cast(inputs[0], tf.float32)\n",
    "#     dec_in = tf.cast(inputs[1], tf.float32)\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    hidden = [tf.zeros((1, 100), dtype=tf.float32)]\n",
    "    feat = tf.cast(inputs[1], tf.float32)\n",
    "#     print(root.shape, feat.shape)\n",
    "    outputs, state, feat = encoder(root, feat, hidden)\n",
    "    \n",
    "    # start of sequence input\n",
    "    start = [0.0 for _ in range(cardinality)]\n",
    "#     start[0] = 1\n",
    "    target_seq = np.array(start).reshape(1, cardinality)\n",
    "    # collect predictions\n",
    "    output = list()\n",
    "#     state = tf.expand_dims(state, 1)\n",
    "    for t in range(n_steps):\n",
    "        # predict next char\n",
    "        \n",
    "        target_seq = tf.cast(target_seq, tf.float32)\n",
    "#         print(target_seq.shape, state.shape, outputs.shape, feat.shape)\n",
    "        yhat, h, att_w = infdec(target_seq, state, outputs, feat)\n",
    "        # store prediction\n",
    "#         print(yhat.shape)\n",
    "        output.append(np.array(yhat))\n",
    "        # update state\n",
    "        state = h\n",
    "        # update target sequence\n",
    "        target_seq = yhat\n",
    "    return np.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "karchch         karchchissageetu \t\t kagetetetetetetetetetetetetetet\n",
      "anqqar          anqqarissaanaageetoo \t\t aratetetetetetetetetetetetetete\n",
      "koshsh          koshshissiiddi \t\t koratetetetetetetetetetetetetet\n",
      "aakim           aakimees \t\t aretetetetetetetetetetetetetete\n",
      "wocam           wocamissiyageetee \t\t wogetetetetetetetetetetetetetet\n",
      "tum             tumikkii \t\t taneretetetetetetetetetetetetet\n",
      "qaatt           qaattsseeta \t\t qararetetetetetetetetetetetetet\n",
      "inott           inottssaanaageetee \t\t iyetetetetetetetetetetetetetete\n",
      "qolett          qolettiyari \t\t qrere\n",
      "yambbar         yambbarissabeekkee \t\t yoretetetetetetetetetetetetetet\n",
      "wuxaawux        wuxaawuxibeettennee \t\t wonatetetetetetetetetetetetetet\n",
      "tikk            tikkageeti \t\t tara\n",
      "xeeray          xeerayissoos \t\t xona\n",
      "quf             qufissibeenna \t\t qaratatetetetetetetetetetetetet\n",
      "sholloorett     sholloorettidi \t\t s\n",
      "wobb            wobbssabeukku \t\t woratate\n",
      "barchchey       barchcheyogeetoo \t\t bogatetetetetetetetetetetetetet\n",
      "possay          possayissaree \t\t praretetetetetetetetetetetetete\n",
      "handdat         handdatoos \t\t h\n",
      "wuttaal         wuttaalissoos \t\t wonatetetetetetetetetetetetetet\n",
      "siray           sirayissidageeti \t\t s\n",
      "markkay         markkayaanaageetoo \t\t mraretetetetetetetetetetetetete\n",
      "acoy            acoyissiyaree \t\t a\n",
      "yaakul          yaakulissiyogeeta \t\t yogetetetetetetetetetetetetetet\n",
      "qoxom           qoxomissidari \t\t qaratatete\n",
      "pirccay         pirccayaas \t\t pyatetetetetetetetetetetetetete\n",
      "achch           achchissibeokkonaa \t\t aratetetetetetetetetetetetetete\n",
      "binjjill        binjjillssuutee \t\t bonatatetetetetetetetetetetetet\n",
      "buskkatt        buskkattu \t\t bonatetetetetetetetete\n",
      "lashilash       lashilashissidogee \t\t lonagetetetetetetetetetetetetet\n",
      "kurum           kurumissada \t\t koretetetetete\n",
      "yuu             yuuissogoo \t\t yogetetetetetetetetetetetetetet\n",
      "lagget          laggetiyogeeta \t\t lregetetetetetetetetetetetetete\n",
      "pakaak          pakaakidogeeta \t\t pgatetetetetetetetetetetetetete\n",
      "sim             simissiyaro \t\t s\n",
      "alleeq          alleeqissibeokkonaa \t\t aratetetetetetetetetetetetetete\n",
      "cecc            ceccibeokkonaa \t\t cragatetetetetetetetetetetetete\n",
      "gin             ginissibeokkonaa \t\t garetetetetetetetetetetetetetet\n",
      "cuul            cuulissiyori \t\t ciera\n",
      "aawat           aawatiyoree \t\t aretetetetetetetetetetetetetete\n",
      "yibidd          yibiddssuutee \t\t yrogetetetetetetetetetetetetete\n",
      "maqqacc         maqqaccissadee \t\t monetatetetetetetetetetetetetet\n",
      "kiirikiir       kiirikiiridagaa \t\t karatatetetetetetetetetetetetet\n",
      "xeell           xeellettays \t\t xana\n",
      "qom''olat       qom''olatidaree \t\t qreretetetetetetetetetetetetete\n",
      "gawx            gawxissi \t\t garetetetekekekekekekekekekekek\n",
      "dabuluuq        dabuluuqissidari \t\t dogetete\n",
      "qayd            qaydissoosona \t\t qregatetete\n",
      "shut            shutissidageeta \t\t s\n",
      "luukk           luukkssiyogeeta \t\t lonagetetetetetetetetetetetetet\n",
      "gagantt         gaganttays \t\t gaagetetetetetetetetetetetetete\n",
      "sooq            sooqaasi \t\t s\n",
      "lo''            lo''abeakka \t\t lona\n",
      "wuyg            wuygissogee \t\t woratatetetekeke\n",
      "bellet          belletagaa \t\t begetetetetetetetetetetetetetet\n",
      "hoshshat        hoshshatidogoo \t\t h\n",
      "haar            haarissideta \t\t h\n",
      "bollot          bollotidanaa \t\t baretetetetetetetetetetetetetet\n",
      "lallab          lallabara \t\t loera\n",
      "azalss          azalssssogeetu \t\t aretetetetetetetetetetetetetete\n",
      "lil''           lil''ogeeti \t\t lro\n",
      "shirqqiiq       shirqqiiqissidogaa \t\t s\n",
      "paah            paahissiiddi \t\t iyagetetete\n",
      "qaaxx           qaaxxssaasa \t\t qyoratatetetetetetetetetetetete\n",
      "irzziiz         irzziizissoppo \t\t i\n",
      "qoox            qooxageetu \t\t qaaratetetetetetetetetetetetete\n",
      "lanqq           lanqqssuutee \t\t lretetetetetetetetetetetetetete\n",
      "carkk           carkkssidi \t\t cyogatetetetetetetetetetetetete\n",
      "zom             zomissidogee \t\t zoratatetetetetetetetetetetetet\n",
      "korett          korettssidogoo \t\t kogetetetetetetetetetetetetetet\n",
      "mankkat         mankkatissennee \t\t monatetetetetetetetetetetetetet\n",
      "duubbal         duubbalissidageeta \t\t dogatetetetetetetetetetetetetet\n",
      "corppopp        corppoppssaanaagaa \t\t crogetetetetetetetetetetetetete\n",
      "tiicitiic       tiicitiicissoosona \t\t tragetetetetetetetetetetetetete\n",
      "suur            suuribeekketii \t\t s\n",
      "darcc           darccara \t\t dogetetetetetete\n",
      "kerbbebb        kerbbebbssogee \t\t kogatetetetetetetetetetetetetet\n",
      "tas             tasagaa \t\t taaragere\n",
      "maay            maayays \t\t m\n",
      "mikiik          mikiikissaanaageetee \t\t monatatetetetetetetetetetetetet\n",
      "qaamm           qaammidonii \t\t qra\n",
      "sorphphay       sorphphayidoree \t\t sretetetetetetetetetetetetetete\n",
      "mer             merissaydaa \t\t maretatetetetetetetetetetetetet\n",
      "cadd            caddaasa \t\t cyotetetetete\n",
      "dadd            daddssari \t\t dogetetetetetetetetetetetetetet\n",
      "puush           puusho \t\t ppatetetete\n",
      "cimm            cimmssettenna \t\t croratatetetetetetetetetete\n",
      "hiillat         hiillatikkinaa \t\t hretetetetetetetetetetetetetete\n",
      "mah             mahissogaa \t\t migatetetetetetetetetetetetetet\n",
      "qodd            qoddagee \t\t qaere\n",
      "zirimot         zirimotadee \t\t zaragatete\n",
      "xill            xillssageeti \t\t xeratetetetetetetetetetetetetet\n",
      "hallacc         hallaccissiyari \t\t h\n",
      "qinjjiqinjj     qinjjiqinjjibeokko \t\t qaenatetetetetetetetetetetetete\n",
      "worddot         worddotay \t\t wdretetetetetetetetetetetetetet\n",
      "yeemott         yeemottogeetu \t\t yore\n",
      "yedis           yedisokko \t\t yoretetetetetetetetetetete\n",
      "qac             qacissibeettennee \t\t qaratatetetetetetetetetetetetet\n",
      "xaazat          xaazatiyogoo \t\t xaaaratatetetetetetetetetetetet\n",
      "didigaay        didigaayissi \t\t dogatetetetetetetetetetetetetet\n",
      "cigg            ciggaanaaro \t\t cyateteteke\n",
      "boshett         boshettogee \t\t bogetetetetetetetetetetetetetet\n",
      "haddir          haddirissiyageetoo \t\t h\n",
      "sheenott        sheenottogeeta \t\t s\n",
      "kafaaf          kafaafidari \t\t koratatekete\n",
      "yabacatt        yabacattssiyageeta \t\t yogetetetetetetetetetetetetetet\n",
      "qeex            qeexissibeettennee \t\t qeratatetetetetetetetetetetetet\n",
      "siyett          siyettssiyageetee \t\t s\n",
      "cozhzhatt       cozhzhattokkonaa \t\t cretetetetetetetetetetetetetete\n",
      "kixx            kixxssuuteetii \t\t konetetetetetetetetetetetetetet\n",
      "xum             xumissuuteetii \t\t x\n",
      "umbbubul        umbbubulissoppona \t\t u\n",
      "haaxx           haaxxokkona \t\t hretetetetetete\n",
      "loytt           loyttssidogaa \t\t lreeragetetetetetetetetetetetet\n",
      "bunjjibunjj     bunjjibunjjaanaari \t\t bonatetetetetetetetetetetetetet\n",
      "gis             gisissidetii \t\t geretetetetetetetetetetetetetet\n",
      "kinnaal         kinnaalabeikkinaa \t\t koratatetetetetetetetetetetetet\n",
      "xikkill         xikkilliyori \t\t xonatate\n",
      "wozannaam       wozannaamibeenna \t\t wona\n",
      "corppopp        corppoppiyogeeta \t\t cretetetetetetetetetetetetetete\n",
      "zeer            zeerikkinaa \t\t zoratatetetetetetetetetetetetet\n",
      "bar             baridageetu \t\t begetetetetetetetete\n",
      "tim             timadee \t\t traragatetetetetetetetetetetete\n",
      "shachchatt      shachchattssiyoro \t\t s\n",
      "shaf            shafissoro \t\t s\n",
      "dantt           danttssi \t\t donatetetetete\n",
      "amaassal        amaassalissidagee \t\t aretetetetetetetetetetetetetete\n",
      "suur            suurissidagee \t\t s\n",
      "azzan           azzanissogeetee \t\t aretetetetetetetetetetetetetete\n",
      "hayzz           hayzzssagoo \t\t hre\n",
      "zub             zubibeenna \t\t zaratatetetetetetetetete\n",
      "duchchis        duchchisiyonaa \t\t dogatetetetetetetetetetetetetet\n",
      "cuccum          cuccumissidari \t\t crera\n",
      "ammantt         ammanttageetoo \t\t aretetetetetetetetetetetetetete\n",
      "xiikk           xiikkssideta \t\t xoratate\n",
      "kawyy           kawyyssidi \t\t kogetetetetetetetetetetetetetet\n",
      "gurub           gurubabeikkinaa \t\t garatetetetetetetetetetetetetet\n",
      "anah            anahissaro \t\t aretetetetetetetetetetetetetete\n",
      "pinccaam        pinccaamissori \t\t priragatetetetetetetetetetetete\n",
      "tistt           tistteeta \t\t trare\n",
      "darax           daraxissees \t\t dogatetetetetetetetetetetetetet\n",
      "bookk           bookkssokkona \t\t boratatetetetetete\n",
      "buuqq           buuqqiyogee \t\t begetetetetetetetetetetetetetet\n",
      "phenqqesh       phenqqeshissagaa \t\t preratetetetetetetetetetetetete\n",
      "patt            pattssibeennee \t\t piratatetetetetetetetetetetetet\n",
      "ganddigaar      ganddigaarissaanaaree \t\t garetetetetetetetetetetetetetet\n",
      "gufann          gufannssenna \t\t garetetetetetetetetetetetetetet\n",
      "dabbakk         dabbakkiyageeti \t\t dogetetetetetetetetetetetetetet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bunjjibunjj     bunjjibunjjibeokkonaa \t\t boratetetetetetetetetetetetetet\n",
      "piqq            piqqssidoro \t\t pregetetetetetetetetetetetetete\n",
      "qem''elatt      qem''elattidoree \t\t qreretetetetetetetetetetetetete\n",
      "maggulss        maggulssiyogeeti \t\t miratetetetetetetetetetetetetet\n",
      "ticicatt        ticicattabeikkinaa \t\t taaratatetetetetetetetetetetete\n",
      "dancc           danccissiyogeeta \t\t dogetetetetetetetetetetetetetet\n",
      "tifilatt        tifilattidoree \t\t tanaretatetetetetetetetetetetet\n",
      "qoobbat         qoobbatanee \t\t qragetetetetetetetetetetetetete\n",
      "yiillot         yiillotuutee \t\t yratetetetetetetetetetetetetete\n",
      "karshsh         karshshissidosona \t\t kora\n",
      "darccuntt       darccunttssiyogee \t\t dogetetetetetetetetetetetetetet\n",
      "qaar            qaarissoppo \t\t qaratatetetetetetetetetetetetet\n",
      "qirxxat         qirxxatennee \t\t qregetetetetetetetetetetetetete\n",
      "corbbobay       corbbobayidara \t\t crogatetetetetetetetetetetetete\n",
      "macimachch      macimachchissaree \t\t monatatetetetetetetetetetetetet\n",
      "shookk          shookkssokkona \t\t s\n",
      "caal            caalidara \t\t crotatetete\n",
      "sha'            sha'iyara \t\t s\n",
      "teeshshott      teeshshottidaree \t\t tararetatetetetetetetetetetetet\n",
      "zaqqull         zaqqullaree \t\t zoratatetetetetetetetetetetetet\n",
      "qulpp           qulppssaanaagee \t\t qraretetetetetetetetetetetetete\n",
      "xudd            xuddaas \t\t xraretetetetekeke\n",
      "hoommot         hoommotissaas \t\t h\n",
      "kool            koolissadii \t\t koretetetetetetetetetetetetetet\n",
      "pix             pixidogeeta \t\t pratetetetetetetetetetetetetete\n",
      "kutuut          kutuutara \t\t koratatetetetetetetetetetetetet\n",
      "zaar            zaarettabeekkee \t\t zaratatetetetetetetetete\n",
      "kaalot          kaalotissaasu \t\t koratetetetetetetetetetetetetet\n",
      "phalqq          phalqqabeikke \t\t pregetetetetetetetetetetetetete\n",
      "toochch         toochchissiyagee \t\t toretatetetetetetetetetetetetet\n",
      "moog            moogissekkee \t\t moratatetetetetetetetetetetetet\n",
      "tul             tulidagee \t\t taaalaratatetetetetetetetetetet\n",
      "awaajj          awaajjssadasa \t\t aratetetetetetetetetetetetetete\n",
      "aass            aassssidogeetee \t\t aretetetetetetetetetetetetetete\n",
      "guxun           guxunageetoo \t\t garatatetetetetetetetetetetetet\n",
      "tiicitiic       tiicitiicissokkonii \t\t tiratetetetetetetetetetetetetet\n",
      "wodd            woddssidaro \t\t woratete\n",
      "qacinchch       qacinchchissibeokkonaa \t\t qaratatetetetetetetetetetetetet\n",
      "y               yanne \t\t yoratetetetetetekekekekekekekek\n",
      "gam''           gam''ssaanaageeti \t\t garatatetetetetetetetetetetetet\n",
      "jarijar         jarijariyageeti \t\t jrlaragetetetetetetetetetetetet\n",
      "golddodatt      golddodattiyo \t\t gare\n",
      "gufann          gufannssuuteetii \t\t giratatetetetetetetetetetetetet\n",
      "mereer          mereerissageetu \t\t maratetetetetetetetetetetetetet\n",
      "qorccay         qorccayissiyaro \t\t qaragetetetetetetetetetetetetet\n",
      "dechch          dechchari \t\t dogetetetetetetetetetetetetetet\n",
      "qiib            qiibissora \t\t qyeratatetekekeke\n",
      "dawtt           dawttokkonaa \t\t donatetetetetetetetetetetetetet\n",
      "gam''           gam''okkonaa \t\t garagatetetetetetetetetetetetet\n",
      "xubb            xubbssidogaa \t\t xaratate\n",
      "toshshay        toshshayissa \t\t tareragatetetetete\n",
      "cuul            cuulissoppite \t\t cieeera\n",
      "shikkaal        shikkaalissiyogeetoo \t\t s\n",
      "qolchch         qolchchidageetoo \t\t qragetetetetetetetetetetetetete\n",
      "borss           borssssiyori \t\t begeteteteke\n",
      "m               miiyaree \t\t m\n",
      "haattat         haattatissoo \t\t h\n",
      "xomoos          xomoosissidageeta \t\t xoratatetetetetetetetetetetetet\n",
      "caal            caalissageeta \t\t creretetetetetetetetetetetetete\n",
      "phixxill        phixxillidagee \t\t pgagatetetetetetetetetetetetete\n",
      "gaytigayt       gaytigaytogeeti \t\t geretetetetetetetetetetetetetet\n",
      "podday          poddayissaanaara \t\t pregetetetetetetetetetetetetete\n",
      "garmaam         garmaamidogoo \t\t gara\n",
      "awaajj          awaajjssidonii \t\t aretetetetetetetetetetetetetete\n",
      "kidhidhdhot     kidhidhdhotissaree \t\t kogetetetetetetetetetetetetetet\n",
      "kulum''ett      kulum''ettibeokkonii \t\t koratetetetetetetetetetetetetet\n",
      "acot            acota \t\t aretetetetetete\n",
      "mees            meesiyonii \t\t moratatetetetetetetetetetetetet\n",
      "tuulam          tuulamokkonii \t\t troragetetetetetetetetetetetete\n",
      "shikkibill      shikkibillssada \t\t s\n",
      "kajjot          kajjotissidogeeta \t\t kogetetetetetetetetetetetetetet\n",
      "lil''           lil''ssabeikkii \t\t loratatetetetetetetetetetetetet\n",
      "ucc             uccissidogeetu \t\t u\n",
      "good            goodadasa \t\t garetatetetetetetetetetetetetet\n",
      "maar            maarissays \t\t maretatetetekekekekekekekekekek\n",
      "borinashsh      borinashshissikkinaa \t\t bogetetetetetetetetetetetetetet\n",
      "pedukk          pedukko \t\t ppatetetetetete\n",
      "heemm           heemmssageeti \t\t h\n",
      "xam             xamissiyori \t\t xoratetekeke\n",
      "qiphiqiph       qiphiqiphissabeikke \t\t qaratatetetetetetetetetetetetet\n",
      "mirqq           mirqqadee \t\t m\n",
      "lokkatt         lokkattssay \t\t l\n",
      "antt            anttssogoo \t\t aratetetetetetetetetetetetetete\n",
      "demm            demmidara \t\t dogetete\n",
      "shaaramux       shaaramuxidori \t\t s\n",
      "tobb            tobbidagaa \t\t taaragateke\n",
      "eess            eessakka \t\t ere\n",
      "qoomm           qoommssageetoo \t\t qararetetetetetetetetetetetetet\n",
      "kilppay         kilppayissiyanaa \t\t kdna\n",
      "gad             gadoppa \t\t gareteteteteteke\n",
      "possay          possayidari \t\t uragatetetetetete\n",
      "payd            paydissaanaagoo \t\t pyatatetetetetetetetetetetetete\n",
      "kool            koolibeokko \t\t koratetekekeke\n",
      "camm            cammssageetee \t\t creratatetetetetetetetetetetete\n",
      "giirett         giirettssiyonii \t\t garatatetetetetetetetetetetetet\n",
      "saamett         saamettaanaagoo \t\t s\n",
      "walass          walassssabeakka \t\t woratetetetetetetetetetetetetet\n",
      "aduqq           aduqqibeenna \t\t aratetetetetetetetetetetetetete\n",
      "guupp           guuppssaasu \t\t garatatetete\n",
      "qulxxuxx        qulxxuxxageetoo \t\t qragetetetetetetetetetetetetete\n",
      "zinqq           zinqqoppa \t\t ziretatetetetetetetete\n",
      "dirqqall        dirqqallssadee \t\t dogatetetetetetetetetetetetetet\n",
      "gaan            gaanidageeti \t\t geretetetetetetetetetetetetetet\n",
      "gepp            geppanai \t\t gaareteteteteke\n",
      "hulluuq         hulluuqissaanaaree \t\t h\n",
      "qof             qofissora \t\t qaeragatetekeke\n",
      "daadulmat       daadulmatissoppo \t\t dogetetetetetetetetetetetetetet\n",
      "gaasoy          gaasoyissidori \t\t gere\n",
      "pol             polettoppo \t\t ppa\n",
      "kelkk           kelkkssettenna \t\t kogatetetetetetetete\n",
      "bal             baliyagee \t\t begeteteteteteteteteteteteke\n",
      "shirqqiiq       shirqqiiqissibeekketii \t\t soratetetetetetetetetetetetetet\n",
      "agur            aguriyageeta \t\t areteteteteteteteke\n",
      "dagay           dagayageetee \t\t dogatetetetetetetetetetetetetet\n",
      "sharishar       sharisharissido \t\t s\n",
      "tuqq            tuqqssogeeta \t\t tiretatetetetetetetetetetetetet\n",
      "icc             iccissidageetoo \t\t i\n",
      "gophphatt       gophphattssaasi \t\t garetetetetete\n",
      "yabiyab         yabiyabissida \t\t yogatetetetetetetetetetetetetet\n",
      "cittay          cittayissite \t\t cretetetetetetetetetetetetetete\n",
      "xorkkot         xorkkotidagoo \t\t xrere\n",
      "ashot           ashotida \t\t aretetetetetetetetetetetetetete\n",
      "kaxot           kaxotidosona \t\t kooragetetetetetetetetetetetete\n",
      "gindd           ginddssidageeta \t\t geretetetetetetetetetetetetetet\n",
      "seebaanett      seebaanettidageetu \t\t sretetetetetetetetetetetetetete\n",
      "hoshshat        hoshshatissageetu \t\t h\n",
      "birxxott        birxxottssiyoree \t\t begetetetetetetetetetetetetetet\n",
      "azzan           azzanissidageetu \t\t aretetetetetetetetetetetetetete\n",
      "torqqott        torqqottidagee \t\t taanaratatetetetetetetetetetete\n",
      "ambboom         ambboomona \t\t aratetetetetetetetetetetetetete\n",
      "wontt           wonttokkonaa \t\t wonragetetetetetetetetetekeke\n",
      "daadir          daadirissideta \t\t dogetetetetetetetetetetetetetet\n",
      "yaynn           yaynnabeakka \t\t yoretetetetetetetetetetetetetet\n",
      "loq             loqokkonaa \t\t lregetetetetetetetetetetetetete\n",
      "kawyy           kawyyssageetee \t\t kegetetetetetetetetetetetetetet\n",
      "sulumuum        sulumuumibeokkonii \t\t s\n",
      "guurumm         guurummssaanaaro \t\t garatetetetetetetetetetetetetet\n",
      "xum             xumissogeeta \t\t xoragerekekekekekekekekekeke\n",
      "loggom          loggomogee \t\t lreragatetetetetetetetetetetete\n",
      "yuud            yuudiyageeti \t\t yoretetetetetetetekekekekekeke\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-7d5f6fcd028a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# predicts the target word given root word and features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mroot_word_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_input_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.replace('&', ' ')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.replace('&', ' ')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-fc0b36816eff>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(infenc, infdec, inputs, n_steps, cardinality)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtarget_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#         print(target_seq.shape, state.shape, outputs.shape, feat.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matt_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfdec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;31m# store prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#         print(yhat.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amany\\anaconda2\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 660\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-02a38bb1a878>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, hidden, enc_output, feat)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# output shape == (batch_size, vocab)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;31m#         print(x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amany\\anaconda2\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 660\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amany\\anaconda2\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1007\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mixed_precision_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_cast_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1009\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amany\\anaconda2\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6038\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6039\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6040\u001b[1;33m         transpose_a, \"transpose_b\", transpose_b)\n\u001b[0m\u001b[0;32m   6041\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6042\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# shows sample examples and calculates accuracy\n",
    "\n",
    "total, correct = 0, 0\n",
    "in_word = 0\n",
    "sims = []\n",
    "for b in range(test_n_batches):\n",
    "    # get data from test data generator\n",
    "    [X1, X2, X3], y = next(test_gen)\n",
    "    for j in range(test_batch_size):\n",
    "        word_features = X3[j].reshape((1, X3.shape[1])) \n",
    "        root_word_matrix = X1[j].reshape((1, X1.shape[1], X1.shape[2]))\n",
    "#         word_index = X4[j].reshape((1, X4.shape[1]))\n",
    "        # predicts the target word given root word and features\n",
    "        \n",
    "        target = predict(encoder, decoder, [root_word_matrix, word_features], n_steps_out, n_input_length)\n",
    "        root = ''.join(dg.one_hot_decode(X1[j]))#.replace('&', ' ')\n",
    "        word = ''.join(dg.one_hot_decode(y[j]))#.replace('&', ' ')\n",
    "        targetS = ''.join(dg.one_hot_decode(target))#.replace('&', ' ')\n",
    "#         sims.append(dg.word_sim(word, targetS))\n",
    "        \n",
    "        # checks if the predicted and the real words are equal\n",
    "        if dg.one_hot_decode(y[j]) == dg.one_hot_decode(target):\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(root, word.split('&')[0], '\\t\\t', targetS.split('&')[0])\n",
    "#         if root.strip() in targetS.strip():\n",
    "#             in_word += 1\n",
    "#     print(b, root, word, targetS)\n",
    "    total += test_batch_size\n",
    "    \n",
    "\n",
    "print('Exact Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
