{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from data_gen import *\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "enc_units = 512\n",
    "feat_unit = 15\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, enc_units, feat_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(feat_units, activation=\"relu\", name=\"feature_output\")\n",
    "        self.fc2 = tf.keras.layers.Dense(enc_units, activation=\"relu\", name=\"state_out\")\n",
    "        \n",
    "    def call(self, w, f, hidden):\n",
    "        output, state = self.gru(w, initial_state=hidden)\n",
    "        feat = self.fc1(f)\n",
    "        state = tf.concat([state, feat], axis=1)\n",
    "        state = self.fc2(state)\n",
    "        return output, state, feat\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder(enc_units, feat_unit, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_x = [np.random.rand(batch_size, 15, 20).astype(np.float64), np.random.rand(batch_size, 32).astype(np.float64)]\n",
    "# sample_hidden = encoder.initialize_hidden_state()\n",
    "# s = tf.cast(sample_x[0], tf.float32)\n",
    "# k = tf.cast(sample_x[1], tf.float32)\n",
    "# sample_output, sample_hidden, sample_feat = encoder(s, k, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_layer = BahdanauAttention(10)\n",
    "# attention_result, attention_weights = attention_layer(sample_hidden, sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(28, activation=\"softmax\")\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output, feat):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = tf.concat([context_vector, x, feat], axis=-1)\n",
    "        x = tf.expand_dims(x, 1)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        x = self.fc(output)\n",
    "        return x, state#, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(enc_units, batch_size)\n",
    "encoder = Encoder(enc_units, feat_unit, batch_size)\n",
    "# sample_decoder_output, _, _ = decoder(tf.random.uniform((batch_size,  29)), sample_hidden, sample_output, sample_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(root, dec_input, feature, target, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden, feat = encoder(root, feature, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        for t in range(target.shape[1]):\n",
    "            predictions, dec_hidden = decoder(dec_input[:, t], dec_hidden, enc_output, feat)\n",
    "            loss += loss_function(target[:, t], predictions)\n",
    "\n",
    "        batch_loss = (loss / int(target.shape[1]))\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# data generator\n",
    "dg = DataGen(data=\"data/wol-aligned.txt\")\n",
    "\n",
    "# length of a word\n",
    "n_input_length = len(char2int)\n",
    "n_steps_in = dg.max_root_len\n",
    "n_steps_out = dg.max_output_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train data:  12898\n",
      "Steps: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Total train data: \", int(len(dg.words) * .016))\n",
    "batch_size = 128\n",
    "# number of batches to train\n",
    "n_batches = int(len(dg.words) * .016 / batch_size) \n",
    "print(\"Steps: {0}\".format(n_batches))\n",
    "# python generator to generate training data at each request\n",
    "# E.x word_matrix, feature = next(gen)\n",
    "gen = dg.rnn_gen_data(batch_size=batch_size, n_batches=n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.3337\n",
      "Epoch 1 Batch 10 Loss 1.7042\n",
      "Epoch 1 Batch 20 Loss 1.3817\n",
      "Epoch 1 Batch 30 Loss 1.3378\n",
      "Epoch 1 Batch 40 Loss 1.2658\n",
      "Epoch 1 Batch 50 Loss 1.1767\n",
      "Epoch 1 Batch 60 Loss 1.2030\n",
      "Epoch 1 Batch 70 Loss 1.0482\n",
      "Epoch 1 Batch 80 Loss 1.0387\n",
      "Epoch 1 Batch 90 Loss 0.9496\n",
      "Epoch 1 Loss 1.2895\n",
      "Time taken for 1 epoch 144.61152362823486 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.8608\n",
      "Epoch 2 Batch 10 Loss 0.8523\n",
      "Epoch 2 Batch 20 Loss 0.7776\n",
      "Epoch 2 Batch 30 Loss 0.7285\n",
      "Epoch 2 Batch 40 Loss 0.6450\n",
      "Epoch 2 Batch 50 Loss 0.6054\n",
      "Epoch 2 Batch 60 Loss 0.5830\n",
      "Epoch 2 Batch 70 Loss 0.4863\n",
      "Epoch 2 Batch 80 Loss 0.4895\n",
      "Epoch 2 Batch 90 Loss 0.4205\n",
      "Epoch 2 Loss 0.6232\n",
      "Time taken for 1 epoch 68.56032228469849 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.3732\n",
      "Epoch 3 Batch 10 Loss 0.4159\n",
      "Epoch 3 Batch 20 Loss 0.3630\n",
      "Epoch 3 Batch 30 Loss 0.3069\n",
      "Epoch 3 Batch 40 Loss 0.2554\n",
      "Epoch 3 Batch 50 Loss 0.2697\n",
      "Epoch 3 Batch 60 Loss 0.2616\n",
      "Epoch 3 Batch 70 Loss 0.2077\n",
      "Epoch 3 Batch 80 Loss 0.1870\n",
      "Epoch 3 Batch 90 Loss 0.1749\n",
      "Epoch 3 Loss 0.2739\n",
      "Time taken for 1 epoch 66.48948454856873 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1532\n",
      "Epoch 4 Batch 10 Loss 0.1496\n",
      "Epoch 4 Batch 20 Loss 0.1653\n",
      "Epoch 4 Batch 30 Loss 0.1345\n",
      "Epoch 4 Batch 40 Loss 0.1170\n",
      "Epoch 4 Batch 50 Loss 0.1147\n",
      "Epoch 4 Batch 60 Loss 0.1334\n",
      "Epoch 4 Batch 70 Loss 0.2855\n",
      "Epoch 4 Batch 80 Loss 0.2512\n",
      "Epoch 4 Batch 90 Loss 0.1841\n",
      "Epoch 4 Loss 0.1835\n",
      "Time taken for 1 epoch 66.23618912696838 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1484\n",
      "Epoch 5 Batch 10 Loss 0.1363\n",
      "Epoch 5 Batch 20 Loss 0.1336\n",
      "Epoch 5 Batch 30 Loss 0.1102\n",
      "Epoch 5 Batch 40 Loss 0.0952\n",
      "Epoch 5 Batch 50 Loss 0.0922\n",
      "Epoch 5 Batch 60 Loss 0.0984\n",
      "Epoch 5 Batch 70 Loss 0.0796\n",
      "Epoch 5 Batch 80 Loss 0.0855\n",
      "Epoch 5 Batch 90 Loss 0.0732\n",
      "Epoch 5 Loss 0.1040\n",
      "Time taken for 1 epoch 66.19309592247009 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0720\n",
      "Epoch 6 Batch 10 Loss 0.0676\n",
      "Epoch 6 Batch 20 Loss 0.0797\n",
      "Epoch 6 Batch 30 Loss 0.0595\n",
      "Epoch 6 Batch 40 Loss 0.0540\n",
      "Epoch 6 Batch 50 Loss 0.0591\n",
      "Epoch 6 Batch 60 Loss 0.0623\n",
      "Epoch 6 Batch 70 Loss 0.0700\n",
      "Epoch 6 Batch 80 Loss 0.0550\n",
      "Epoch 6 Batch 90 Loss 0.0473\n",
      "Epoch 6 Loss 0.0616\n",
      "Time taken for 1 epoch 65.53941941261292 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0487\n",
      "Epoch 7 Batch 10 Loss 0.0427\n",
      "Epoch 7 Batch 20 Loss 0.0556\n",
      "Epoch 7 Batch 30 Loss 0.0353\n",
      "Epoch 7 Batch 40 Loss 0.0328\n",
      "Epoch 7 Batch 50 Loss 0.0359\n",
      "Epoch 7 Batch 60 Loss 0.0390\n",
      "Epoch 7 Batch 70 Loss 0.0345\n",
      "Epoch 7 Batch 80 Loss 0.0336\n",
      "Epoch 7 Batch 90 Loss 0.0277\n",
      "Epoch 7 Loss 0.0379\n",
      "Time taken for 1 epoch 65.07840609550476 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0275\n",
      "Epoch 8 Batch 10 Loss 0.0265\n",
      "Epoch 8 Batch 20 Loss 0.0436\n",
      "Epoch 8 Batch 30 Loss 0.0221\n",
      "Epoch 8 Batch 40 Loss 0.0201\n",
      "Epoch 8 Batch 50 Loss 0.0201\n",
      "Epoch 8 Batch 60 Loss 0.0231\n",
      "Epoch 8 Batch 70 Loss 0.0235\n",
      "Epoch 8 Batch 80 Loss 0.0167\n",
      "Epoch 8 Batch 90 Loss 0.0180\n",
      "Epoch 8 Loss 0.0242\n",
      "Time taken for 1 epoch 67.3196849822998 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0162\n",
      "Epoch 9 Batch 10 Loss 0.3617\n",
      "Epoch 9 Batch 20 Loss 0.1655\n",
      "Epoch 9 Batch 30 Loss 0.0963\n",
      "Epoch 9 Batch 40 Loss 0.0546\n",
      "Epoch 9 Batch 50 Loss 0.0459\n",
      "Epoch 9 Batch 60 Loss 0.0450\n",
      "Epoch 9 Batch 70 Loss 0.0300\n",
      "Epoch 9 Batch 80 Loss 0.0261\n",
      "Epoch 9 Batch 90 Loss 0.0233\n",
      "Epoch 9 Loss 0.1017\n",
      "Time taken for 1 epoch 65.14990282058716 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0238\n",
      "Epoch 10 Batch 10 Loss 0.0242\n",
      "Epoch 10 Batch 20 Loss 0.0311\n",
      "Epoch 10 Batch 30 Loss 0.0168\n",
      "Epoch 10 Batch 40 Loss 0.0160\n",
      "Epoch 10 Batch 50 Loss 0.0157\n",
      "Epoch 10 Batch 60 Loss 0.0184\n",
      "Epoch 10 Batch 70 Loss 0.0165\n",
      "Epoch 10 Batch 80 Loss 0.0117\n",
      "Epoch 10 Batch 90 Loss 0.0129\n",
      "Epoch 10 Loss 0.0185\n",
      "Time taken for 1 epoch 68.40192484855652 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0131\n",
      "Epoch 11 Batch 10 Loss 0.0145\n",
      "Epoch 11 Batch 20 Loss 0.0236\n",
      "Epoch 11 Batch 30 Loss 0.0118\n",
      "Epoch 11 Batch 40 Loss 0.0101\n",
      "Epoch 11 Batch 50 Loss 0.0104\n",
      "Epoch 11 Batch 60 Loss 0.0120\n",
      "Epoch 11 Batch 70 Loss 0.0119\n",
      "Epoch 11 Batch 80 Loss 0.0081\n",
      "Epoch 11 Batch 90 Loss 0.0071\n",
      "Epoch 11 Loss 0.0124\n",
      "Time taken for 1 epoch 66.97747159004211 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0098\n",
      "Epoch 12 Batch 10 Loss 0.0108\n",
      "Epoch 12 Batch 20 Loss 0.0180\n",
      "Epoch 12 Batch 30 Loss 0.0090\n",
      "Epoch 12 Batch 40 Loss 0.0066\n",
      "Epoch 12 Batch 50 Loss 0.0068\n",
      "Epoch 12 Batch 60 Loss 0.0091\n",
      "Epoch 12 Batch 70 Loss 0.0102\n",
      "Epoch 12 Batch 80 Loss 0.0067\n",
      "Epoch 12 Batch 90 Loss 0.0058\n",
      "Epoch 12 Loss 0.0095\n",
      "Time taken for 1 epoch 67.12963771820068 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0070\n",
      "Epoch 13 Batch 10 Loss 0.0083\n",
      "Epoch 13 Batch 20 Loss 0.0143\n",
      "Epoch 13 Batch 30 Loss 0.0068\n",
      "Epoch 13 Batch 40 Loss 0.0064\n",
      "Epoch 13 Batch 50 Loss 0.0055\n",
      "Epoch 13 Batch 60 Loss 0.0069\n",
      "Epoch 13 Batch 70 Loss 0.0075\n",
      "Epoch 13 Batch 80 Loss 0.0050\n",
      "Epoch 13 Batch 90 Loss 0.0066\n",
      "Epoch 13 Loss 0.0081\n",
      "Time taken for 1 epoch 67.78770041465759 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0055\n",
      "Epoch 14 Batch 10 Loss 0.0062\n",
      "Epoch 14 Batch 20 Loss 0.0119\n",
      "Epoch 14 Batch 30 Loss 0.0067\n",
      "Epoch 14 Batch 40 Loss 0.0066\n",
      "Epoch 14 Batch 50 Loss 0.0052\n",
      "Epoch 14 Batch 60 Loss 0.0049\n",
      "Epoch 14 Batch 70 Loss 0.0069\n",
      "Epoch 14 Batch 80 Loss 0.0035\n",
      "Epoch 14 Batch 90 Loss 0.0033\n",
      "Epoch 14 Loss 0.0060\n",
      "Time taken for 1 epoch 67.64659857749939 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0039\n",
      "Epoch 15 Batch 10 Loss 0.0044\n",
      "Epoch 15 Batch 20 Loss 0.0093\n",
      "Epoch 15 Batch 30 Loss 0.0074\n",
      "Epoch 15 Batch 40 Loss 0.0052\n",
      "Epoch 15 Batch 50 Loss 0.0056\n",
      "Epoch 15 Batch 60 Loss 0.0045\n",
      "Epoch 15 Batch 70 Loss 0.0052\n",
      "Epoch 15 Batch 80 Loss 0.0030\n",
      "Epoch 15 Batch 90 Loss 0.0029\n",
      "Epoch 15 Loss 0.0050\n",
      "Time taken for 1 epoch 64.10735011100769 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0032\n",
      "Epoch 16 Batch 10 Loss 0.0041\n",
      "Epoch 16 Batch 20 Loss 0.0080\n",
      "Epoch 16 Batch 30 Loss 0.0060\n",
      "Epoch 16 Batch 40 Loss 0.0047\n",
      "Epoch 16 Batch 50 Loss 0.0064\n",
      "Epoch 16 Batch 60 Loss 0.0038\n",
      "Epoch 16 Batch 70 Loss 0.0041\n",
      "Epoch 16 Batch 80 Loss 0.0034\n",
      "Epoch 16 Batch 90 Loss 0.0028\n",
      "Epoch 16 Loss 0.0044\n",
      "Time taken for 1 epoch 64.46393609046936 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0031\n",
      "Epoch 17 Batch 10 Loss 0.0033\n",
      "Epoch 17 Batch 20 Loss 0.0155\n",
      "Epoch 17 Batch 30 Loss 0.1255\n",
      "Epoch 17 Batch 40 Loss 0.0666\n",
      "Epoch 17 Batch 50 Loss 0.0296\n",
      "Epoch 17 Batch 60 Loss 0.0311\n",
      "Epoch 17 Batch 70 Loss 0.0144\n",
      "Epoch 17 Batch 80 Loss 0.0122\n",
      "Epoch 17 Batch 90 Loss 0.0080\n",
      "Epoch 17 Loss 0.0414\n",
      "Time taken for 1 epoch 64.49411654472351 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0088\n",
      "Epoch 18 Batch 10 Loss 0.0093\n",
      "Epoch 18 Batch 20 Loss 0.0148\n",
      "Epoch 18 Batch 30 Loss 0.0074\n",
      "Epoch 18 Batch 40 Loss 0.0065\n",
      "Epoch 18 Batch 50 Loss 0.0051\n",
      "Epoch 18 Batch 60 Loss 0.0070\n",
      "Epoch 18 Batch 70 Loss 0.0060\n",
      "Epoch 18 Batch 80 Loss 0.0044\n",
      "Epoch 18 Batch 90 Loss 0.0038\n",
      "Epoch 18 Loss 0.0073\n",
      "Time taken for 1 epoch 64.56486248970032 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0046\n",
      "Epoch 19 Batch 10 Loss 0.0052\n",
      "Epoch 19 Batch 20 Loss 0.0090\n",
      "Epoch 19 Batch 30 Loss 0.0052\n",
      "Epoch 19 Batch 40 Loss 0.0042\n",
      "Epoch 19 Batch 50 Loss 0.0034\n",
      "Epoch 19 Batch 60 Loss 0.0045\n",
      "Epoch 19 Batch 70 Loss 0.0045\n",
      "Epoch 19 Batch 80 Loss 0.0029\n",
      "Epoch 19 Batch 90 Loss 0.0025\n",
      "Epoch 19 Loss 0.0047\n",
      "Time taken for 1 epoch 64.4903917312622 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0030\n",
      "Epoch 20 Batch 10 Loss 0.0035\n",
      "Epoch 20 Batch 20 Loss 0.0050\n",
      "Epoch 20 Batch 30 Loss 0.0044\n",
      "Epoch 20 Batch 40 Loss 0.0029\n",
      "Epoch 20 Batch 50 Loss 0.0024\n",
      "Epoch 20 Batch 60 Loss 0.0031\n",
      "Epoch 20 Batch 70 Loss 0.0038\n",
      "Epoch 20 Batch 80 Loss 0.0023\n",
      "Epoch 20 Batch 90 Loss 0.0020\n",
      "Epoch 20 Loss 0.0036\n",
      "Time taken for 1 epoch 64.44418144226074 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0022\n",
      "Epoch 21 Batch 10 Loss 0.0027\n",
      "Epoch 21 Batch 20 Loss 0.0032\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step in range(n_batches):\n",
    "        [root, dec_in, feat], y = next(gen)\n",
    "#         root = root.astype(np.float32)# tf.cast(root, tf.float32)\n",
    "#         dec_in = tf.cast(dec_in, tf.float32)\n",
    "#         feat = tf.cast(feat, tf.float32)\n",
    "#         y = tf.cast(y, tf.float32)\n",
    "        batch_loss = train_step(root, dec_in, feat, y, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if step % (n_batches // 10) == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     step,\n",
    "                                                     batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "#     if (epoch + 1) % 2 == 0:\n",
    "#         checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / n_batches))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_n_batches, test_batch_size =  int(len(dg.words) * .00208 / batch_size), batch_size  \n",
    "print(test_n_batches * test_batch_size)\n",
    "# test_n_batches, test_batch_size = 30, 10\n",
    "\n",
    "# data generator for test data\n",
    "test_gen = dg.rnn_gen_data(batch_size=test_batch_size, n_batches=test_n_batches, trainset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(infenc, infdec, inputs, n_steps, cardinality):\n",
    "    # encode\n",
    "    root = tf.cast(inputs[0], tf.float32)\n",
    "#     dec_in = tf.cast(inputs[1], tf.float32)\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    hidden = [tf.zeros((1, enc_units), dtype=tf.float32)]\n",
    "    feat = tf.cast(inputs[1], tf.float32)\n",
    "#     print(root.shape, feat.shape)\n",
    "    outputs, state, feat = encoder(root, feat, None)\n",
    "    \n",
    "    # start of sequence input\n",
    "    start = [0.0 for _ in range(cardinality)]\n",
    "#     start[0] = 1\n",
    "    target_seq = np.array(start).reshape(1, cardinality)\n",
    "    # collect predictions\n",
    "    output = list()\n",
    "#     state = tf.expand_dims(state, 1)\n",
    "    for t in range(n_steps):\n",
    "        # predict next char\n",
    "        \n",
    "        target_seq = tf.cast(target_seq, tf.float32)\n",
    "#         print(target_seq.shape, state.shape, outputs.shape, feat.shape)\n",
    "        yhat, h = decoder(target_seq, state, outputs, feat)\n",
    "        # store prediction\n",
    "#         print(yhat.shape)\n",
    "        output.append(np.array(yhat))\n",
    "        # update state\n",
    "        state = h\n",
    "        # update target sequence\n",
    "        target_seq = yhat\n",
    "    return np.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shows sample examples and calculates accuracy\n",
    "\n",
    "total, correct = 0, 0\n",
    "in_word = 0\n",
    "sims = []\n",
    "for b in range(test_n_batches):\n",
    "    # get data from test data generator\n",
    "    [X1, X2, X3], y = next(test_gen)\n",
    "    for j in range(test_batch_size):\n",
    "        word_features = X3[j].reshape((1, X3.shape[1])) \n",
    "        root_word_matrix = X1[j].reshape((1, X1.shape[1], X1.shape[2]))\n",
    "#         word_index = X4[j].reshape((1, X4.shape[1]))\n",
    "        # predicts the target word given root word and features\n",
    "        \n",
    "        target = predict(encoder, decoder, [root_word_matrix, word_features], n_steps_out, n_input_length)\n",
    "        root = ''.join(dg.one_hot_decode(X1[j]))#.replace('&', ' ')\n",
    "        word = ''.join(dg.one_hot_decode(y[j]))#.replace('&', ' ')\n",
    "        targetS = ''.join(dg.one_hot_decode(target))#.replace('&', ' ')\n",
    "#         sims.append(dg.word_sim(word, targetS))\n",
    "        \n",
    "        # checks if the predicted and the real words are equal\n",
    "        if dg.one_hot_decode(y[j]) == dg.one_hot_decode(target):\n",
    "            correct += 1\n",
    "#         else:\n",
    "#             print(root, word.split('&')[0], '\\t\\t', targetS.split('&')[0])\n",
    "#         if root.strip() in targetS.strip():\n",
    "#             in_word += 1\n",
    "#     print(b, root, word, targetS)\n",
    "    total += test_batch_size\n",
    "    \n",
    "\n",
    "print('Exact Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12898 1664 16  89.66% 82.69% 93.63%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12898 1664 25 93.54 89.84% 89.42%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12898 1664 30 92.61% 93.57% 95.32% 94.35%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12898 1664 30 512 86.84 95.43 94.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12898 1664 40 512 94.23 95.49 96.09"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
