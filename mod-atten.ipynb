{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from data_gen import *\n",
    "import time\n",
    "import preprocess as pre\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--batch_size\", type=int,  default=128)\n",
    "# parser.add_argument(\"--char_embed_size\", type=int,  default=32)\n",
    "# parser.add_argument(\"--feat_embed_size\", type=int, default=32)\n",
    "# parser.add_argument(\"--hidden_size\", type=int, default=265)\n",
    "# parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "# args = parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "char2int, feat2val, max_r, max_w = pre.process(['russian'])\n",
    "# print(feat2val)\n",
    "data = pre.convert(char2int, feat2val, max_r, max_w, langs=['russian'], for_cnn=True)\n",
    "clean_data = pre.convert(char2int, feat2val, max_r, max_w, langs=['russian'], train_set=False, for_cnn=True)\n",
    "gen_data = pre.convert(char2int, feat2val, max_r, max_w, langs=['russian'], train_set=False, for_cnn=True)\n",
    "int2char = {val: key for val, key in enumerate(char2int)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data: 12599 Total Batches 98\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size = 128\n",
    "max_root = max_r + 2\n",
    "max_word = max_w + 2\n",
    "n_feature = data[1].shape[1]\n",
    "hidden_size = 256\n",
    "feat_embed_size = 32\n",
    "char_embed_size = 32\n",
    "EPOCHS = 10\n",
    "n_batches = len(data[0]) // batch_size\n",
    "print(\"Total Data: {0} Total Batches {1}\".format(len(data[0]), n_batches))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, enc_units, feat_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.cnn = tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")\n",
    "        self.pool = tf.keras.layers.MaxPool2D(2, 2)\n",
    "        self.flat = tf.keras.layers.Flatten()\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(feat_units, activation=\"relu\", name=\"feature_output\")\n",
    "        self.fc2 = tf.keras.layers.Dense(enc_units, activation=\"relu\", name=\"state_out\")\n",
    "        \n",
    "    def call(self, w, f):\n",
    "        x = self.cnn(w)\n",
    "        x = self.pool(x)\n",
    "        x = self.flat(x)\n",
    "        feat = self.fc1(f)\n",
    "        state = tf.concat([x, feat], axis=1)\n",
    "        state = self.fc2(state)\n",
    "        return state, feat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, output_size, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform', name=\"decoder_gru\")\n",
    "        self.fc = tf.keras.layers.Dense(output_size, activation=\"softmax\")\n",
    "\n",
    "\n",
    "    def call(self, x, feat, hidden):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        # x = tf.concat([context_vector, x, feat], axis=-1)\n",
    "        x = tf.expand_dims(x, 1)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        x = self.fc(output)\n",
    "        return x, state#, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict(encoder, decoder, inputs, n_steps):\n",
    "    # encode\n",
    "    root, feat = inputs[0], inputs[1]\n",
    "    state, feat = encoder(inputs[0], inputs[1])\n",
    "    \n",
    "    start_word = '<'\n",
    "    start_mat = pre.word_to_matrix(start_word, char2int, 1, ' ')\n",
    "    \n",
    "    target_seq = np.zeros((root.shape[0], len(char2int)), dtype=np.float32) + np.array(start_mat, dtype=np.float32)\n",
    "    outputs = list()\n",
    "    for t in range(n_steps):\n",
    "        # predict next char\n",
    "        target_seq, state = decoder(target_seq, feat, state)\n",
    "        \n",
    "        outputs.append(target_seq)\n",
    "    return np.stack(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "decoder = Decoder(hidden_size, len(char2int), batch_size)\n",
    "encoder = Encoder(hidden_size, feat_embed_size, batch_size)\n",
    "\n",
    "# x = np.random.randn(10, 15, 28,1)\n",
    "# f = np.random.randn(10, 32)\n",
    "# h = tf.cast(np.random.randn(10, 256), tf.float64)\n",
    "# x, f = encoder(x, f)\n",
    "# decoder(x, f, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(root, feature, dec_input, target):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_hidden, feat = encoder(root, feature)\n",
    "        # print(enc_hidden.shape)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        for t in range(target.shape[1]):\n",
    "            predictions, dec_hidden = decoder(dec_input[:, t], feat, dec_hidden)\n",
    "            loss += loss_function(target[:, t], predictions)\n",
    "\n",
    "        batch_loss = (loss / int(target.shape[1]))\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_model(test_data, log=False):\n",
    "    test_n_batches, test_batch_size =  int(test_data[0].shape[0] / batch_size), batch_size  \n",
    "    print(test_n_batches * test_batch_size)\n",
    "    test_gen = pre.gen(data, batch_size)\n",
    "    # shows sample examples and calculates accuracy\n",
    "    test_batches = len(test_data[0]) // batch_size\n",
    "    total, correct = 0, 0\n",
    "    in_word = 0\n",
    "    sims = []\n",
    "    for b in range(test_batches - 1):\n",
    "        # get data from test data generator\n",
    "        [root, feat, dec_in], y = next(test_gen)\n",
    "        root = np.expand_dims(root, axis=3)\n",
    "        pred = predict(encoder, decoder, [root, feat], max_word)\n",
    "        for k in range(pred.shape[1]):\n",
    "            indexes = pred[:, k]#.argmax(axis=1)\n",
    "            r = ''.join(pre.matrix_to_word(root[k], int2char)).strip()[1:-1]\n",
    "            w = ''.join(pre.matrix_to_word(dec_in[k], int2char)).strip()[1:-1]\n",
    "            t = ''.join(pre.matrix_to_word(indexes, int2char)).strip()[:-1]\n",
    "            if w == t:\n",
    "                correct += 1\n",
    "            else:\n",
    "                if log:\n",
    "                    print(r, w, t)\n",
    "\n",
    "\n",
    "        total += batch_size\n",
    "        return float(correct)/float(total)*100.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gen = pre.gen(data, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1792\n",
      "1792\n",
      "Epoch 1 Loss 1.4215 Gen Accuracy 0.0000 Clean Accuracy 0.0000\n",
      "1792\n",
      "1792\n",
      "Epoch 2 Loss 0.8685 Gen Accuracy 0.0000 Clean Accuracy 0.0000\n",
      "1792\n",
      "1792\n",
      "Epoch 3 Loss 0.5733 Gen Accuracy 0.7812 Clean Accuracy 0.0000\n",
      "1792\n",
      "1792\n",
      "Epoch 4 Loss 0.4079 Gen Accuracy 1.5625 Clean Accuracy 2.3438\n",
      "1792\n",
      "1792\n",
      "Epoch 5 Loss 0.3168 Gen Accuracy 10.1562 Clean Accuracy 6.2500\n",
      "1792\n",
      "1792\n",
      "Epoch 6 Loss 0.2596 Gen Accuracy 8.5938 Clean Accuracy 7.8125\n",
      "1792\n",
      "1792\n",
      "Epoch 7 Loss 0.2223 Gen Accuracy 20.3125 Clean Accuracy 16.4062\n",
      "1792\n",
      "1792\n",
      "Epoch 8 Loss 0.1883 Gen Accuracy 18.7500 Clean Accuracy 25.0000\n",
      "1792\n",
      "1792\n",
      "Epoch 9 Loss 0.1652 Gen Accuracy 28.1250 Clean Accuracy 25.0000\n",
      "1792\n",
      "1792\n",
      "Epoch 10 Loss 0.1427 Gen Accuracy 38.2812 Clean Accuracy 34.3750\n",
      "1792\n",
      "удавы удавом удавам\n",
      "цыганского цыганский цыганские\n",
      "уводом уводах уводые\n",
      "противоречиям противоречии противоречие\n",
      "правопорядки правопорядок правопородки\n",
      "христадельфианское христадельфианском христадальфиньномом\n",
      "снизили снизят снизит>\n",
      "приморскую приморском примосском>\n",
      "присоединяясь присоединялась присоединнсся\n",
      "поели поесть поетьть\n",
      "восславляют восславляю восславлюю\n",
      "рискующий рискуй рискай\n",
      "хаяло хайте хаяйи>\n",
      "чешку чешке чешке>\n",
      "заросли зарослям зарослим\n",
      "вызовем вызови вызовае\n",
      "подлаивай подлаиваю подлаваю\n",
      "побеждал побеждаем побеждам\n",
      "притин притина притнн\n",
      "островам острова островы\n",
      "компостированиями компостирование компостирования\n",
      "двойственностям двойственности двойственносте\n",
      "консорциума консорциуме консорциум>\n",
      "тку ткём ткум\n",
      "насыщением насыщении насыщение\n",
      "нектары нектару нектаро\n",
      "буссоль буссоли буссоле\n",
      "примерные примерная примерной\n",
      "рыцари рыцарей рыцари>\n",
      "экзотическим экзотическому экзотическом>\n",
      "морщится морщась морщится\n",
      "веретенообразного веретенообразные веретеноробазаых\n",
      "художественны художественную художественное\n",
      "осознаний осознании осознание\n",
      "валета валетов валетам\n",
      "вноси вносил вноса\n",
      "оправдаешь оправдали оправдалит\n",
      "убери убрали убррил\n",
      "брезгливостей брезгливости брезгливосто\n",
      "упорную упорным упорном\n",
      "терминам терминов терминам\n",
      "метрополитенах метрополитены метрополитова\n",
      "нюхают нюхаю нююаю\n",
      "машинист машиниста машинись\n",
      "хоккей хоккеем хоккем\n",
      "бетонируют бетонируй бетониууу>\n",
      "съедобного съедобном съедобного\n",
      "вчерашних вчерашни вчерашние\n",
      "гомосексуальной гомосексуальные гомосескуааныы>\n",
      "железное железно железное\n",
      "неисправностью неисправности неисправнстос\n",
      "коллизией коллизии коллизие\n",
      "ящику ящики ящиках\n",
      "грабителях грабителю грабители\n",
      "осуществимое осуществимом осуществиммм\n",
      "хвощу хвоща хвощо\n",
      "буссолью буссолей буссолья\n",
      "детёнышах детёнышем детёныша\n",
      "выполотый выполют выполот\n",
      "гафнию гафнии гафние\n",
      "водружайте водружаемый водружаеный\n",
      "манекенами манекене манекенее\n",
      "общины община общины\n",
      "виолончелям виолончели вилоннчель\n",
      "лаврами лавром лаврам\n",
      "хоккеях хоккеи хоккее\n",
      "презервативу презервативом презерзаторам\n",
      "согнула согнёте согнушет\n",
      "носильщик носильщике носильщик\n",
      "проявление проявлений проявления\n",
      "ослепительную ослепительному ослепителеному\n",
      "некрофилов некрофилам некрофилом\n",
      "различишь различивший различиший\n",
      "рефренов рефрены рефренты\n",
      "образованному образованный образованные\n",
      "базисами базисы базиса\n",
      "купавшийся купающийся купавшийся\n",
      "оловянно оловянной ооловнною\n",
      "посвящения посвящений посвящения\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38.28125"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    start = time.time()\n",
    "\n",
    "    # enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step in range(n_batches):\n",
    "        [root, feat, dec_in], y = next(gen)\n",
    "        root = np.expand_dims(root, axis=3)\n",
    "        batch_loss = train_step(root, feat, dec_in, y)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "#         if step % (n_batches // 1) == 0:\n",
    "#             print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "#                                                      step,\n",
    "#                                                      batch_loss.numpy()))\n",
    "    clean_accuracy = test_model(clean_data)\n",
    "    gen_accuracy = test_model(gen_data)\n",
    "    print('Epoch {} Loss {:.4f} Gen Accuracy {:.4f} Clean Accuracy {:.4f}'.format(epoch + 1,total_loss / n_batches, gen_accuracy, clean_accuracy))\n",
    "    \n",
    "test_model(clean_data, log=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
