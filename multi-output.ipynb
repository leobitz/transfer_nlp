{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_gen import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator\n",
    "dg = DataGen(data=\"data/wolaytta-train.txt\")\n",
    "\n",
    "# length of a word\n",
    "n_input_length = len(char2int)\n",
    "n_steps_in = dg.max_root_len\n",
    "n_steps_out = dg.max_output_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10346.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14780 * .7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train data:  561794.1\n"
     ]
    }
   ],
   "source": [
    "14780 * .7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total train data: \", len(dg.words) * .1)\n",
    "batch_size = 128\n",
    "# number of batches to train\n",
    "n_batches = int(len(dg.words) * .7 / batch_size) \n",
    "\n",
    "# python generator to generate training data at each request\n",
    "# E.x word_matrix, feature = next(gen)\n",
    "gen = dg.cnn_gen_data_multi_word(batch_size=batch_size, n_batches=n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infenc - inference encoder model\n",
    "# infdec - inference decoder model\n",
    "# train - training model that combines both\n",
    "# n_input_length - the length of the input and the output\n",
    "# word_feat_len - the length of the word feature vector\n",
    "# n_units - size of the hidden memory in the RNN\n",
    "train, infenc, infdec = conv_multi_model(n_input_length, n_input_length, dg.word_feat_len + 3, 256)\n",
    "train.compile(optimizer='adam', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0721 00:41:20.982088 139671214438208 deprecation.py:323] From /home/leo/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "4389/4389 [==============================] - 161s 37ms/step - loss: 0.1676\n",
      "Epoch 2/4\n",
      "4389/4389 [==============================] - 178s 41ms/step - loss: 0.0121\n",
      "Epoch 3/4\n",
      "4389/4389 [==============================] - 166s 38ms/step - loss: 0.0056\n",
      "Epoch 4/4\n",
      "4389/4389 [==============================] - 156s 35ms/step - loss: 0.0035\n"
     ]
    }
   ],
   "source": [
    "# model train given the data generator, how many batches and number of epochs\n",
    "history = train.fit_generator(gen, steps_per_epoch=n_batches, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "dg2 = DataGen(data=\"data/wolaytta-test.txt\")\n",
    "test_n_batches, test_batch_size =  int(len(dg2.words) * 1. / batch_size), batch_size  \n",
    "# test_n_batches, test_batch_size = 30, 10 \n",
    "\n",
    "# data generator for test data\n",
    "test_gen = dg2.cnn_gen_data_multi_word(batch_size=test_batch_size, n_batches=test_n_batches, trainset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(infenc, infdec, inputs, n_steps, cardinality):\n",
    "    # encode\n",
    "    state = infenc.predict(inputs)\n",
    "    # start of sequence input\n",
    "    start = [0.0 for _ in range(cardinality)]\n",
    "#     start[0] = 1\n",
    "    target_seq = np.array(start).reshape(1, 1, cardinality)\n",
    "    # collect predictions\n",
    "    output = list()\n",
    "    for t in range(n_steps):\n",
    "        # predict next char\n",
    "        yhat, h= infdec.predict([target_seq, state])\n",
    "        # store prediction\n",
    "        output.append(yhat[0,0,:])\n",
    "        # update state\n",
    "        state = h\n",
    "        # update target sequence\n",
    "        target_seq = yhat\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Accuracy: 61.74%\n"
     ]
    }
   ],
   "source": [
    "# shows sample examples and calculates accuracy\n",
    "\n",
    "total, correct = 0, 0\n",
    "in_word = 0\n",
    "sims = []\n",
    "for b in range(test_n_batches):\n",
    "    # get data from test data generator\n",
    "    [X1, X2, X3], y = next(test_gen)\n",
    "    for j in range(test_batch_size):\n",
    "        word_features = X3[j].reshape((1, X3.shape[1])) \n",
    "        root_word_matrix = X1[j].reshape((1, X1.shape[1], X1.shape[2], 1))\n",
    "#         word_index = X4[j].reshape((1, X4.shape[1]))\n",
    "        # predicts the target word given root word and features\n",
    "        target = predict(infenc, infdec, [root_word_matrix, word_features], n_steps_out, n_input_length)\n",
    "#         root = ''.join(dg.one_hot_decode(X1[j]))#.replace('&', ' ')\n",
    "#         word = ''.join(dg.one_hot_decode(y[j]))#.replace('&', ' ')\n",
    "#         targetS = ''.join(dg.one_hot_decode(target))#.replace('&', ' ')\n",
    "#         sims.append(dg.word_sim(word, targetS))\n",
    "        \n",
    "        # checks if the predicted and the real words are equal\n",
    "        if ''.join(dg.one_hot_decode(y[j])).strip() == ''.join(dg.one_hot_decode(target)).strip():\n",
    "            correct += 1\n",
    "#         else:\n",
    "#             print(root, word.split('&')[0], '\\t\\t', targetS.split('&')[0])\n",
    "#         if root.strip() in targetS.strip():\n",
    "#             in_word += 1\n",
    "#     print(b, root, word, targetS)\n",
    "    total += test_batch_size\n",
    "    \n",
    "\n",
    "print('Exact Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tifilatt tifilattaree [0, 1, 0]\n",
      "xoon xoonissogeeta [0, 0, 1]\n",
      "qas qasabeakka [1, 0, 0]\n",
      "baq baqissikkii [1, 0, 0]\n",
      "onccilatt onccilattaasa [1, 0, 0]\n",
      "bogg boggssiyogeetu [1, 0, 0]\n",
      "afuxx afuxxssakka [1, 0, 0]\n",
      "qayc qaycissiyagee [1, 0, 0]\n",
      "teeliteel teeliteelissidori [1, 0, 0]\n",
      "yaat yaatissogeeta [0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# lines = open(\"data/wol-multi.txt\").readlines()\n",
    "for i in range(10):\n",
    "    print(dg.roots[i], dg.words[i], dg.word_indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2feat, word2root= {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in lines:\n",
    "#     line = line[:-1].split(' ')\n",
    "#     word2feat[line[0]] = line[1]\n",
    "#     word2root[line[0]] = line[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check = {}\n",
    "# for word in word2feat:\n",
    "#     root = word2feat[word]\n",
    "#     feat = word2root[word]\n",
    "#     key = root + \" \" + feat\n",
    "#     if key not in check:\n",
    "#         check[key] = []\n",
    "#     check[key].append(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "# file = open(\"data/wol-m.txt\", \"w\")\n",
    "# for key in check.keys():\n",
    "#     words = check[key]\n",
    "#     for i in range(len(words)):\n",
    "#         word = words[i]\n",
    "#         line = \"{0} {1} {2}\\n\".format(word, key, i)\n",
    "#         file.write(line)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
