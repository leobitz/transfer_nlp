{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data_gen import *\n",
    "from models import *\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data generator for gofa word, root word, word feature\n",
    "dg2 = DataGen(data=\"data/goffa.txt\")\n",
    "\n",
    "# length of a word\n",
    "n_input_length = len(char2int)\n",
    "n_steps_in = dg2.max_root_len\n",
    "n_steps_out = dg2.max_output_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"model.h5\")\n",
    "root_input = model.get_layer(\"root_word_input\").output\n",
    "feature_input = model.get_layer(\"word_feature_input\").output\n",
    "target_input = model.get_layer(\"target_word_input\").output\n",
    "\n",
    "state_h = model.get_layer(\"concatnate\").output\n",
    "encoder_model = Model([root_input, feature_input], state_h)\n",
    "\n",
    "\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_gru = model.get_layer(\"decoder_gru\")\n",
    "gru_outputs, state_h  = decoder_gru(target_input, initial_state=decoder_state_input_h)\n",
    "\n",
    "decoder_dense = model.get_layer(\"train_output\")\n",
    "decoder_outputs = decoder_dense(gru_outputs)\n",
    "\n",
    "decoder_model = Model([target_input, decoder_state_input_h], [decoder_outputs, state_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infenc - inference encoder model\n",
    "# infdec - inference decoder model\n",
    "# train - training model that combines both\n",
    "# n_input_length - the length of the input and the output\n",
    "# word_feat_len - the length of the word feature vector\n",
    "# n_units - size of the hidden memory in the RNN\n",
    "# model, encoder_model, decoder_model = conv_model(n_input_length, n_input_length, dg2.word_feat_len, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  8320\n"
     ]
    }
   ],
   "source": [
    "# number of batches for the gofa data generator\n",
    "batch_size = 128\n",
    "n_batches = int(len(dg2.words) * .05 / batch_size) \n",
    "gen2 = dg2.cnn_gen_data(batch_size=batch_size, n_batches=n_batches)\n",
    "print(\"Train size: \", (n_batches * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "65/65 [==============================] - 13s 195ms/step - loss: 0.0974\n",
      "Epoch 2/20\n",
      "65/65 [==============================] - 8s 126ms/step - loss: 0.0570\n",
      "Epoch 3/20\n",
      "65/65 [==============================] - 8s 126ms/step - loss: 0.0555\n",
      "Epoch 4/20\n",
      "65/65 [==============================] - 8s 127ms/step - loss: 0.0548\n",
      "Epoch 5/20\n",
      "65/65 [==============================] - 8s 126ms/step - loss: 0.0544\n",
      "Epoch 6/20\n",
      "65/65 [==============================] - 8s 126ms/step - loss: 0.0539\n",
      "Epoch 7/20\n",
      "65/65 [==============================] - 8s 129ms/step - loss: 0.0536\n",
      "Epoch 8/20\n",
      "65/65 [==============================] - 8s 126ms/step - loss: 0.0534\n",
      "Epoch 9/20\n",
      "65/65 [==============================] - 8s 128ms/step - loss: 0.0534\n",
      "Epoch 10/20\n",
      "65/65 [==============================] - 8s 126ms/step - loss: 0.0534\n",
      "Epoch 11/20\n",
      "65/65 [==============================] - 8s 126ms/step - loss: 0.0420\n",
      "Epoch 12/20\n",
      "65/65 [==============================] - 8s 128ms/step - loss: 0.0381\n",
      "Epoch 13/20\n",
      "65/65 [==============================] - 8s 126ms/step - loss: 0.0270\n",
      "Epoch 14/20\n",
      "65/65 [==============================] - 8s 129ms/step - loss: 0.0191\n",
      "Epoch 15/20\n",
      "65/65 [==============================] - 8s 126ms/step - loss: 0.0182\n",
      "Epoch 16/20\n",
      "65/65 [==============================] - 8s 127ms/step - loss: 0.0181\n",
      "Epoch 17/20\n",
      "65/65 [==============================] - 8s 130ms/step - loss: 0.0180\n",
      "Epoch 18/20\n",
      "65/65 [==============================] - 8s 129ms/step - loss: 0.0268\n",
      "Epoch 19/20\n",
      "65/65 [==============================] - 8s 127ms/step - loss: 0.0187\n",
      "Epoch 20/20\n",
      "65/65 [==============================] - 8s 125ms/step - loss: 0.0182\n"
     ]
    }
   ],
   "source": [
    "# model train given the data generator, how many batches and number of epochs\n",
    "history = model.fit_generator(gen2, steps_per_epoch=n_batches, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  3328\n"
     ]
    }
   ],
   "source": [
    "# test data generator for gofa words\n",
    "\n",
    "g_test_batches = int(len(dg2.words) * .02 / batch_size) \n",
    "gen3 = dg2.cnn_gen_data(batch_size=batch_size, n_batches=g_test_batches, trainset=False)\n",
    "print(\"Train size: \", (g_test_batches * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piig            piigokkonii \t\t piigettennee\n",
      "tooc            toocettennee \t\t toocokkonii\n",
      "munaqq          munaqqadinaa \t\t munaqqadii\n",
      "zawr            zawradinaa \t\t zawradii\n",
      "shanggoc        shanggocokkonii \t\t shanggocettennee\n",
      "happurssaa      happurssaaoppite \t\t happurrsamoppite\n",
      "higishshiball   higishshiballabiikke \t\t hiishishablabaakke\n",
      "kar             karadinaa \t\t karadii\n",
      "meesh           meeshettennee \t\t meeshokkonii\n",
      "yaayyam         yaayyamettennee \t\t yaayyamokkonii\n",
      "z               zideta \t\t zidee\n",
      "nashsh          nashshanee \t\t nashshonee\n",
      "paah            paahettennee \t\t paahokkonii\n",
      "keten''         keten''okkonii \t\t keten''ettennee\n",
      "shoppatt        shoppattadinaa \t\t shoppattadii\n",
      "wonzz           wonzzadinaa \t\t wonzzadii\n",
      "dummat          dummatokkonii \t\t dummatettennee\n",
      "piikkatt        piikkattii \t\t piikkatteetii\n",
      "waaqiwaaq       waaqiwaaqadinaa \t\t waaqiwaaqadii\n",
      "yaayyam         yaayyamadinaa \t\t yaayyamadii\n",
      "sig             sigettennee \t\t sigekkenii\n",
      "koyroy          koyroyadinaa \t\t koyroyadii\n",
      "hiillat         hiillatettennee \t\t hiillatokkonii\n",
      "annaac          annaacanee \t\t annaaconee\n",
      "biz''           biz''okkonii \t\t biz''ettennee\n",
      "xiilley         xiilleyokkonii \t\t xiilleyettennee\n",
      "bashsh          bashshokkonii \t\t bashshoktenii\n",
      "gus             gusanee \t\t gusonee\n",
      "hooranss        hooranssanee \t\t hooranssonee\n",
      "param           paramadinaa \t\t paramadii\n",
      "arac            aracokkonii \t\t aracettennee\n",
      "uu              uuikke \t\t uuekka\n",
      "holjjojj        holjjojjettennee \t\t holjjojjokkonii\n",
      "harbbuuq        harbbuuqanee \t\t harbbuuqoneetee\n",
      "baabis          baabisettennee \t\t baabisokkonii\n",
      "pultt           pulttadinaa \t\t pulttadii\n",
      "s               sadinaa \t\t sasi\n",
      "xoon            xoonadinaa \t\t xoonadii\n",
      "jawl            jawlokkonii \t\t jawlettennee\n",
      "xambbixall      xambbixalladee \t\t xambbaxalladee\n",
      "bawt            bawtanee \t\t bawtonee\n",
      "qoh             qohanee \t\t qohonee\n",
      "halaalet        halaaletettennee \t\t halaaletekketii\n",
      "kuur            kuurettennee \t\t kuurokkonii\n",
      "donzz           donzzanee \t\t donzzonee\n",
      "pushakk         pushakkadinaa \t\t pushakkadii\n",
      "utt             uttettennee \t\t uttokkonii\n",
      "diiq            diiqadinaa \t\t diiqadii\n",
      "higishshiball   higishshiballees \t\t hiishishaallees\n",
      "hassay          hassayadinaa \t\t hassayadii\n",
      "diiq            diiqettennee \t\t diiqokkonii\n",
      "balgg           balggadinaa \t\t balggadii\n",
      "kurum           kurumanee \t\t kurumoneetee\n",
      "suugg           suuggokkonii \t\t suuggekkonii\n",
      "aymott          aymottadinaa \t\t aymottadii\n",
      "s               sadee \t\t sadai\n",
      "pooxaa          pooxaaettennee \t\t pooxaaokkonii\n",
      "higishshiball   higishshiballidee \t\t hiishishaalldii\n",
      "kuuy            kuuyanee \t\t kuuyoppo\n",
      "qocir           qociradinaa \t\t qociradii\n",
      "siy             siyikke \t\t siyenna\n",
      "caaqq           caaqqanee \t\t caaqqoneetee\n",
      "yigg            yiggokkonii \t\t yiggettennee\n",
      "booxx           booxxanee \t\t booxxoneetee\n",
      "baq             baqokkonii \t\t baqettennee\n",
      "yexx            yexxokkonii \t\t yexxettennee\n",
      "damaam          damaamokkonii \t\t damaamettennee\n",
      "piikkatt        piikkattadinaa \t\t piikkattadii\n",
      "qelxxumm        qelxxummanee \t\t qelxxummoneetee\n",
      "qer             qerettennee \t\t qerokkonii\n",
      "dechch          dechchadinaa \t\t dechchadii\n",
      "naaqq           naaqqettennee \t\t naaqqokkonii\n",
      "nakk            nakkokkonii \t\t nakkettennee\n",
      "baar            baarettennee \t\t baarokkonii\n",
      "coray           corayadinaa \t\t corayadii\n",
      "baa'ul          baa'ulanee \t\t baa'ulonee\n",
      "gogg            gogganee \t\t goggonee\n",
      "keemm           keemmanee \t\t keemmonee\n",
      "murtt           murttanee \t\t murttonee\n",
      "keel            keelettennee \t\t keelokkonii\n",
      "xaagg           xaaggokkonii \t\t xaaggettennee\n",
      "lakakk          lakakkadinaa \t\t lakakkadii\n",
      "cooy            cooyadinaa \t\t cooyadii\n",
      "y               yibeekketa \t\t yynnee\n",
      "baylibayl       baylibaylokkonii \t\t baylibaylettennee\n",
      "amaassal        amaassaladinaa \t\t amaassaladii\n",
      "luukk           luukkettennee \t\t luukkokkonii\n",
      "paax            paaxettennee \t\t paaxokkonii\n",
      "shimpp          shimppynaa \t\t shimppyy\n",
      "cash            cashokkonii \t\t cashettennee\n",
      "ordd            orddanee \t\t orddonee\n",
      "inddetaasu      inddetaasuikkii \t\t indddtaasuikkii\n",
      "qin''arett      qin''arettettennee \t\t qin''arettokkonii\n",
      "ooshsh          ooshshanee \t\t ooshshonee\n",
      "canggaraar      canggaraarettennee \t\t canggaraarokkonii\n",
      "terzzezz        terzzezzokko \t\t terzzezzekketa\n",
      "loq             loqadinaa \t\t loqadii\n",
      "naaqq           naaqqadinaa \t\t naaqqadii\n",
      "doliwaaq        doliwaaqanee \t\t doliwaaqonee\n",
      "daannat         daannatettennee \t\t daannatokkonii\n",
      "halgg           halggokkonii \t\t halggettennee\n",
      "hal             haladinaa \t\t haladii\n",
      "pentt           penttadinaa \t\t penttadii\n",
      "soldd           solddokkonii \t\t solddettennee\n",
      "shol''          shol''okkonii \t\t shol''ettennee\n",
      "ushsh           ushshadinaa \t\t ushshadii\n",
      "z               zynaa \t\t zays\n",
      "yooqechch       yooqechchadinaa \t\t yooqechchadii\n",
      "garbball        garbballanee \t\t garbballonee\n",
      "xambbixall      xambbixallennee \t\t xambbaxallennee\n",
      "ho''            ho''adinaa \t\t ho''adii\n",
      "d               dees \t\t diss\n",
      "tum             tumokkonii \t\t tumekkonii\n",
      "xambbixall      xambbixallibeenna \t\t xambbaxallibeenna\n",
      "ahum            ahumadinaa \t\t ahumadii\n",
      "seexiseex       seexiseexadinaa \t\t seexiseexadii\n",
      "walall          walalladinaa \t\t walalladii\n",
      "mor             morettennee \t\t morokkonii\n",
      "dulkk           dulkkokkonii \t\t dulkkettennee\n",
      "osh             oshadinaa \t\t oshadii\n",
      "kiy             kiyanee \t\t kiyonee\n",
      "s               sa \t\t sab a\n",
      "sug             sugadinaa \t\t sugadii\n",
      "loggom          loggomanee \t\t loggomonee\n",
      "wuuqq           wuuqqadinaa \t\t wuuqqadii\n",
      "hoosh           hooshadinaa \t\t hooshadii\n",
      "abaraad         abaraadaanee \t\t araraadaanee\n",
      "phopholqqatt    phopholqqattettennee \t\t phopholqqattokkonii\n",
      "tuggay          tuggayadinaa \t\t tuggayadii\n",
      "sholloorett     sholloorettokkonii \t\t sholloorettettennee\n",
      "eexx            eexxadinaa \t\t eexxadii\n",
      "noskkay         noskkayadinaa \t\t noskkayadii\n",
      "cogochch        cogochchakka \t\t cocochchakka\n",
      "hiillat         hiillatadinaa \t\t hiillatadii\n",
      "pixir           pixiranee \t\t pixironee\n",
      "takkaar         takkaaranee \t\t takkaaronee\n",
      "shoobb          shoobbokkonii \t\t shoobbettennee\n",
      "takkaar         takkaaradinaa \t\t takkaaradii\n",
      "lepp            leppettennee \t\t leppokkonii\n",
      "cibibi          cibibiadinaa \t\t cibibiadii\n",
      "kaxot           kaxotettennee \t\t kaxotokkonii\n",
      "dinnaaq         dinnaaqanee \t\t dinnaaqonee\n",
      "Exact Accuracy: 95.73%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# shows sample examples and calculates accuracy\n",
    "\n",
    "total, correct = 0, 0\n",
    "in_word = 0\n",
    "sims = []\n",
    "for b in range(g_test_batches):\n",
    "    # get data from test data generator\n",
    "    [X1, X2, X3], y = next(gen3)\n",
    "    for j in range(batch_size):\n",
    "        word_features = X3[j].reshape((1, X3.shape[1])) \n",
    "        root_word_matrix = X1[j].reshape((1, X1.shape[1], X1.shape[2], 1))\n",
    "        \n",
    "        # predicts the target word given root word and features\n",
    "        target = predict(encoder_model, decoder_model, root_word_matrix, word_features, n_steps_out, n_input_length)\n",
    "        root = ''.join(dg2.one_hot_decode(X1[j]))#.replace('&', ' ')\n",
    "        word = ''.join(dg2.one_hot_decode(y[j]))#.replace('&', ' ')\n",
    "        targetS = ''.join(dg2.one_hot_decode(target))#.replace('&', ' ')\n",
    "#         sims.append(dg.word_sim(word, targetS))\n",
    "        \n",
    "        # checks if the predicted and the real words are equal'\n",
    "#         print(len(dg.one_hot_decode(y[j])), len(dg.one_hot_decode(target)))\n",
    "#         print(len(dg.one_hot_decode(target)[:27]))\n",
    "#         print(len(y[j]))\n",
    "        if dg2.one_hot_decode(y[j]) == dg2.one_hot_decode(target)[:27]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(root, word.split('&')[0], '\\t\\t', targetS.split('&')[0])\n",
    "        if root.strip() in targetS.strip():\n",
    "            in_word += 1\n",
    "#     print(b, root, word, targetS)\n",
    "    total += batch_size\n",
    "    \n",
    "\n",
    "print('Exact Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97.57, 97.69, 97.54, 97.48, 97.72, 97.36, 97.51]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[97.57, 97.69, 97.54, 97.48, 97.72, 97.36, 97.51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0974451454786154,\n",
       " 0.057033732246894106,\n",
       " 0.05549450046741045,\n",
       " 0.054814370119800934,\n",
       " 0.05444869072391437,\n",
       " 0.053862272919370575,\n",
       " 0.05356800304009364,\n",
       " 0.053443708557348986,\n",
       " 0.05340436138212681,\n",
       " 0.053390522071948415,\n",
       " 0.04202403320142856,\n",
       " 0.03813541886898188,\n",
       " 0.026991238874884752,\n",
       " 0.019109935481817678,\n",
       " 0.018226230074651538,\n",
       " 0.01814174318480162,\n",
       " 0.018007075392121735,\n",
       " 0.026805303765174288,\n",
       " 0.018683376427865227,\n",
       " 0.01816844432346093]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.63"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.08649586516504104,\n",
    " 0.00981702023687271,\n",
    " 0.0034727409643192705,\n",
    " 0.002293258377064306,\n",
    " 0.0017179189742399523,\n",
    " 0.0013772513368166984,\n",
    " 0.0011395107838325202,\n",
    " 0.000961381251601359,\n",
    " 0.0008240110254309212,\n",
    " 0.0007225332513021735]\n",
    "\n",
    "[1.6368019525821393,\n",
    " 1.109701026402987,\n",
    " 0.8467150734021114,\n",
    " 0.6373297223678002,\n",
    " 0.4980540312253512,\n",
    " 0.3955386955004472,\n",
    " 0.30929824755741997,\n",
    " 0.24970667981184447,\n",
    " 0.2069551529792639,\n",
    " 0.17557245813883268,\n",
    " 0.18030325586979207,\n",
    " 0.14884408918710856,\n",
    " 0.12150984463783411,\n",
    " 0.10766618079864061,\n",
    " 0.09909856835236916,\n",
    " 0.08846385880158497,\n",
    " 0.07828886463091923,\n",
    " 0.07907314289074678,\n",
    " 0.06622060502950962,\n",
    " 0.05161330436284726]\n",
    "46.63\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to literal (<ipython-input-12-fcfaffb2fed0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-fcfaffb2fed0>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    \"similarity\" = 0.7878232042064027\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't assign to literal\n"
     ]
    }
   ],
   "source": [
    "\"similarity\" = 0.7878232042064027"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
