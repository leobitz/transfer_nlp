{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data_gen import *\n",
    "from models import *\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data generator for gofa word, root word, word feature\n",
    "dg2 = DataGen(data=\"data/goffa.txt\")\n",
    "\n",
    "# length of a word\n",
    "n_input_length = len(char2int)\n",
    "n_steps_in = dg2.max_root_len\n",
    "n_steps_out = dg2.max_output_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"model.h5\")\n",
    "root_input = model.get_layer(\"root_word_input\").output\n",
    "feature_input = model.get_layer(\"word_feature_input\").output\n",
    "target_input = model.get_layer(\"target_word_input\").output\n",
    "\n",
    "state_h = model.get_layer(\"concatnate\").output\n",
    "encoder_model = Model([root_input, feature_input], state_h)\n",
    "\n",
    "\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_gru = model.get_layer(\"decoder_gru\")\n",
    "gru_outputs, state_h  = decoder_gru(target_input, initial_state=decoder_state_input_h)\n",
    "\n",
    "decoder_dense = model.get_layer(\"train_output\")\n",
    "decoder_outputs = decoder_dense(gru_outputs)\n",
    "\n",
    "decoder_model = Model([target_input, decoder_state_input_h], [decoder_outputs, state_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infenc - inference encoder model\n",
    "# infdec - inference decoder model\n",
    "# train - training model that combines both\n",
    "# n_input_length - the length of the input and the output\n",
    "# word_feat_len - the length of the word feature vector\n",
    "# n_units - size of the hidden memory in the RNN\n",
    "# model, encoder_model, decoder_model = conv_model(n_input_length, n_input_length, dg2.word_feat_len, 256)\n",
    "# train.compile(optimizer='adam', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  8320\n"
     ]
    }
   ],
   "source": [
    "# number of batches for the gofa data generator\n",
    "batch_size = 128\n",
    "n_batches = int(len(dg2.words) * .05 / batch_size) \n",
    "gen2 = dg2.cnn_gen_data(batch_size=batch_size, n_batches=n_batches)\n",
    "print(\"Train size: \", (n_batches * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.0945\n",
      "Epoch 2/8\n",
      "65/65 [==============================] - 5s 73ms/step - loss: 0.0560\n",
      "Epoch 3/8\n",
      "65/65 [==============================] - 5s 73ms/step - loss: 0.0480\n",
      "Epoch 4/8\n",
      "65/65 [==============================] - 5s 74ms/step - loss: 0.0385\n",
      "Epoch 5/8\n",
      "65/65 [==============================] - 5s 74ms/step - loss: 0.0377\n",
      "Epoch 6/8\n",
      "65/65 [==============================] - 5s 76ms/step - loss: 0.0373\n",
      "Epoch 7/8\n",
      "65/65 [==============================] - 5s 74ms/step - loss: 0.0372\n",
      "Epoch 8/8\n",
      "65/65 [==============================] - 5s 73ms/step - loss: 0.0371\n"
     ]
    }
   ],
   "source": [
    "# model train given the data generator, how many batches and number of epochs\n",
    "history = model.fit_generator(gen2, steps_per_epoch=n_batches, epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  3328\n"
     ]
    }
   ],
   "source": [
    "# test data generator for gofa words\n",
    "\n",
    "g_test_batches = int(len(dg2.words) * .02 / batch_size) \n",
    "gen3 = dg2.cnn_gen_data(batch_size=batch_size, n_batches=g_test_batches, trainset=False)\n",
    "print(\"Train size: \", (g_test_batches * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Accuracy: 93.09%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# shows sample examples and calculates accuracy\n",
    "\n",
    "total, correct = 0, 0\n",
    "in_word = 0\n",
    "sims = []\n",
    "for b in range(g_test_batches):\n",
    "    # get data from test data generator\n",
    "    [X1, X2, X3], y = next(gen3)\n",
    "    for j in range(batch_size):\n",
    "        word_features = X3[j].reshape((1, X3.shape[1])) \n",
    "        root_word_matrix = X1[j].reshape((1, X1.shape[1], X1.shape[2], 1))\n",
    "        \n",
    "        # predicts the target word given root word and features\n",
    "        target = predict(encoder_model, decoder_model, root_word_matrix, word_features, n_steps_out, n_input_length)\n",
    "        root = ''.join(dg2.one_hot_decode(X1[j]))#.replace('&', ' ')\n",
    "        word = ''.join(dg2.one_hot_decode(y[j]))#.replace('&', ' ')\n",
    "        targetS = ''.join(dg2.one_hot_decode(target))#.replace('&', ' ')\n",
    "#         sims.append(dg.word_sim(word, targetS))\n",
    "        \n",
    "        # checks if the predicted and the real words are equal'\n",
    "#         print(len(dg.one_hot_decode(y[j])), len(dg.one_hot_decode(target)))\n",
    "#         print(len(dg.one_hot_decode(target)[:27]))\n",
    "#         print(len(y[j]))\n",
    "        if dg2.one_hot_decode(y[j]) == dg2.one_hot_decode(target)[:27]:\n",
    "            correct += 1\n",
    "#         else:\n",
    "#             print(root, word.split('&')[0], '\\t\\t', targetS.split('&')[0])\n",
    "        if root.strip() in targetS.strip():\n",
    "            in_word += 1\n",
    "#     print(b, root, word, targetS)\n",
    "    total += batch_size\n",
    "    \n",
    "\n",
    "print('Exact Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
